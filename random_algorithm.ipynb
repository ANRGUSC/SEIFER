{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from keras.applications import *\n",
    "import networkx as nx\n",
    "from tensorflow import keras\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Partitioner:\n",
    "    def __init__(self, model: keras.Model):\n",
    "        self.model = model\n",
    "        self.Stack = []\n",
    "        self.visited = {}\n",
    "        # The \"depth\"/level that a certain layer is at\n",
    "        self.layer_level = {}\n",
    "        # The layers at a certain depth/level, where the index of the array is the level\n",
    "        self.levels = []\n",
    "\n",
    "    def get_previous(self, layer_name):\n",
    "        inbound = self.model.get_layer(layer_name).inbound_nodes[0].inbound_layers\n",
    "        if type(inbound) != list:\n",
    "            inbound = [inbound]\n",
    "        return [layer.name for layer in inbound]\n",
    "\n",
    "    def get_next(self, layer_name):\n",
    "        outbound = self.model.get_layer(layer_name).outbound_nodes\n",
    "        return [node.outbound_layer.name for node in outbound]\n",
    "\n",
    "    # Traverses the model starting from layer_name all the way to start\n",
    "    def traverse(self, layer_name, start, part_name, inpt):\n",
    "        # On subsequent recursive steps, the new input layer will be defined,\n",
    "        # so that name needs to be checked in base case\n",
    "        if (layer_name == start) or (layer_name == part_name):\n",
    "            return inpt\n",
    "\n",
    "        output = []\n",
    "        for n in self.get_previous(layer_name):\n",
    "            output.append(self.traverse(n, start, part_name, inpt))\n",
    "\n",
    "        # If the DAG node only has 1 previous connection\n",
    "        if len(output) == 1:\n",
    "            output = output[0]\n",
    "\n",
    "        layer = self.model.get_layer(layer_name)\n",
    "        to_next = layer(output)\n",
    "        return to_next\n",
    "\n",
    "    def construct_model(self, start, end, part_name=\"part_begin\"):\n",
    "        inpt = keras.Input(tensor=self.model.get_layer(start).output, name=part_name)\n",
    "        output = self.traverse(end, start, part_name, inpt)\n",
    "        part = keras.Model(inputs=self.model.get_layer(start).output, outputs=output)\n",
    "        return part\n",
    "\n",
    "    # TODO write this function\n",
    "    def create_model_partitions(self, node_capacities: List[str], communication_graph: nx.Graph):\n",
    "        node_partition_names = self.partition_model(node_capacities, communication_graph)\n",
    "        model_partitions = {}\n",
    "        for k in node_partition_names:\n",
    "            start_layer, end_layer = node_partition_names[k]\n",
    "            model = self.construct_model(start_layer, end_layer)\n",
    "            model_partitions[k] = model\n",
    "            print(\"Model constructed\")\n",
    "\n",
    "        return model_partitions\n",
    "\n",
    "\n",
    "    # A recursive function used by longest_path. See below\n",
    "    # link for details\n",
    "    # https:#www.geeksforgeeks.org/topological-sorting/\n",
    "    def topological_sort_util(self, v: str):\n",
    "        self.visited[v] = True\n",
    "\n",
    "        # Recur for all the vertices adjacent to this vertex\n",
    "        # list<AdjListNode>::iterator i\n",
    "        for i in self.get_next(v):\n",
    "            if not self.visited[i]:\n",
    "                self.topological_sort_util(i)\n",
    "\n",
    "        # Push current vertex to stack which stores topological\n",
    "        # sort\n",
    "        self.Stack.append(v)\n",
    "\n",
    "    # The function to find longest distances from a given vertex.\n",
    "    # It uses recursive topologicalSortUtil() to get topological\n",
    "    # sorting.\n",
    "    def longest_path(self, s: str) -> List[List[str]]:\n",
    "        for l in self.model.layers:\n",
    "            self.visited[l.name] = False\n",
    "            self.layer_level[l.name] = -1 # Equal to -infty\n",
    "\n",
    "        # Call the recursive helper function to store Topological\n",
    "        # Sort starting from all vertices one by one\n",
    "        for l in self.model.layers:\n",
    "            if not self.visited[l.name]:\n",
    "                self.topological_sort_util(l.name)\n",
    "\n",
    "        # Initialize distances to all vertices as infinite and\n",
    "        # distance to source as 0\n",
    "        self.layer_level[s] = 0\n",
    "\n",
    "        # Process vertices in topological order\n",
    "        while len(self.Stack) > 0:\n",
    "\n",
    "            # Get the next vertex from topological order\n",
    "            u = self.Stack.pop()\n",
    "\n",
    "            # Update distances of all adjacent vertices\n",
    "            # list<AdjListNode>::iterator i\n",
    "            if self.layer_level[u] != -1:\n",
    "                for i in self.get_next(u):\n",
    "                    if self.layer_level[i] < self.layer_level[u] + 1:\n",
    "                        self.layer_level[i] = self.layer_level[u] + 1 # Each edge weighted 1\n",
    "\n",
    "        # Create array of calculated longest distances to layer\n",
    "        layers_at_level = [[]] * len(self.layer_level)\n",
    "        for l in self.model.layers:\n",
    "            if len(layers_at_level[self.layer_level[l.name]]) == 0:\n",
    "                layers_at_level[self.layer_level[l.name]] = []\n",
    "\n",
    "            layers_at_level[self.layer_level[l.name]].append(l.name)\n",
    "\n",
    "        return layers_at_level\n",
    "\n",
    "    def find_singletons(self):\n",
    "        # Model only has 1 input, which is input_names[0]\n",
    "        name = self.model.input_names[0]\n",
    "        # Finding the longest path from the start to every other layer\n",
    "        self.levels = self.longest_path(name)\n",
    "        singletons = []\n",
    "        for l in range(len(self.levels)):\n",
    "            if len(self.levels[l]) == 1:\n",
    "                singletons.append(self.levels[l][0])\n",
    "        return singletons\n",
    "\n",
    "    def find_all_paths_util(self, u, d, visited, path, all_paths):\n",
    "        # If the distance of the current path is greater than the longest path (the \"level\") to the destination node, we know the destination node can't be a partition point\n",
    "        if self.layer_level[u] > self.layer_level[d]:\n",
    "            return False\n",
    "        # Mark the current node as visited and store in path\n",
    "        visited[u] = True\n",
    "        path.append(u)\n",
    "\n",
    "        # If current vertex is same as destination, then print\n",
    "        # current path[] (because we've found a path from u to d)\n",
    "        if u == d:\n",
    "            exists = False\n",
    "            # See if path already exists in list of paths\n",
    "            for p in all_paths:\n",
    "                if p == path:\n",
    "                    exists = True\n",
    "                    break\n",
    "\n",
    "            if not exists:\n",
    "                all_paths.append(path.copy())\n",
    "        else:\n",
    "            # If current vertex is not destination\n",
    "            # Recur for all the vertices adjacent to this vertex\n",
    "            for i in self.get_next(u):\n",
    "                if not visited[i]:\n",
    "                    ret = self.find_all_paths_util(i, d, visited, path, all_paths)\n",
    "                    if not ret:\n",
    "                        return False\n",
    "\n",
    "        # Remove current vertex from path[] and mark it as unvisited\n",
    "        path.pop()\n",
    "        visited[u] = False\n",
    "        return True\n",
    "\n",
    "    # Finds all paths from 's' to 'd.' Returns false if a there exists a path from s that has a greater \"level\" than d, otherwise returns true\n",
    "    def find_all_paths(self, s, d) -> bool:\n",
    "        # Mark all the vertices as not visited\n",
    "        visited = {}\n",
    "        for l in self.model.layers:\n",
    "            visited[l.name] = False\n",
    "\n",
    "        # Create an array to store paths\n",
    "        path = []\n",
    "        all_paths = []\n",
    "\n",
    "        # Call the recursive helper function to find all paths\n",
    "        return self.find_all_paths_util(s, d, visited, path, all_paths)\n",
    "\n",
    "    def partitions_util(self, prev, singleton_nodes, partitions):\n",
    "        # Reached the end of the model and found all the partitions\n",
    "        if len(singleton_nodes) == 0:\n",
    "            return partitions\n",
    "        p = False\n",
    "        i = -1 # So first i starts at 0\n",
    "        # Starting from the previous partition point, we iterate through all the subsequent singleton nodes to find the next partition point\n",
    "        while not p:\n",
    "            i += 1\n",
    "            p = self.find_all_paths(prev, singleton_nodes[i])\n",
    "\n",
    "        partitions.append(singleton_nodes[i])\n",
    "        return self.partitions_util(singleton_nodes[i], singleton_nodes[i + 1:], partitions)\n",
    "\n",
    "    def find_partitions(self) -> List[str]:\n",
    "        inpt = self.model.input_names[0]\n",
    "        return self.partitions_util(inpt, self.find_singletons(), [])\n",
    "\n",
    "    def keras_model_memory_usage_in_bytes(self, model, batch_size: int):\n",
    "        \"\"\"\n",
    "        Return the estimated memory usage of a given Keras model in bytes.\n",
    "        This includes the model weights and layers, but excludes the dataset.\n",
    "\n",
    "        The model shapes are multiplied by the batch size, but the weights are not.\n",
    "\n",
    "        Args:\n",
    "            model: A Keras model.\n",
    "            batch_size: The batch size you intend to run the model with. If you\n",
    "                have already specified the batch size in the model itself, then\n",
    "                pass `1` as the argument here.\n",
    "        Returns:\n",
    "            An estimate of the Keras model's memory usage in bytes.\n",
    "\n",
    "        \"\"\"\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.Model):\n",
    "                internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                    layer, batch_size=batch_size\n",
    "                )\n",
    "            single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n",
    "            out_shape = layer.output_shape\n",
    "            if isinstance(out_shape, list):\n",
    "                out_shape = out_shape[0]\n",
    "            for s in out_shape:\n",
    "                if s is None:\n",
    "                    continue\n",
    "                single_layer_mem *= s\n",
    "            shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def keras_layer_memory(self, layer_name, batch_size: int):\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "\n",
    "        if isinstance(layer_name, tf.keras.Model):\n",
    "            internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                layer_name, batch_size=batch_size\n",
    "            )\n",
    "        single_layer_mem = tf.as_dtype(layer_name.dtype or default_dtype).size\n",
    "        out_shape = layer_name.output_shape\n",
    "        if isinstance(out_shape, list):\n",
    "            out_shape = out_shape[0]\n",
    "        for s in out_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def find_partition_memory(self, partition_points):\n",
    "        part_mems = []\n",
    "        #Each index represents the memory between that part pt and the next one\n",
    "        for i in range(1, len(partition_points)):\n",
    "            # Going backwards along layers within partition to find total memory usage\n",
    "            start = self.layer_level[partition_points[i]]\n",
    "            end = self.layer_level[partition_points[i - 1]]\n",
    "            mem = 0\n",
    "            for j in range(start, end, -1):\n",
    "                for l in self.levels[j]:\n",
    "                    layer_mem = self.keras_layer_memory(self.model.get_layer(l), 1)\n",
    "                    mem += layer_mem\n",
    "            part_mems.append(mem)\n",
    "        # Nothing used after last partition pt, which is output layer\n",
    "        part_mems.append(0)\n",
    "        return part_mems\n",
    "\n",
    "    # Returns transfer size of partition in Mbits\n",
    "    def find_partition_transfer_size(self, partition_points) -> Tuple[List[int], Dict[str, int]]:\n",
    "        transfer_sizes = []\n",
    "        transfer_size_dict = {}\n",
    "        for i in range(len(partition_points)):\n",
    "            num_outbound = len(self.model.get_layer(partition_points[i]).outbound_nodes)\n",
    "\n",
    "            # Iterate through all elements of shape tuple except first one (which is batch size)\n",
    "            output_size = 1\n",
    "            for s in self.model.get_layer(partition_points[i]).get_output_at(0).get_shape()[1:]:\n",
    "                output_size *= s\n",
    "            # Compression ratio is ~1.44 (according to https://www.researchgate.net/publication/264417607_Fixed-Rate_Compressed_Floating-Point_Arrays)\n",
    "            zfp_comp_ratio = 1.44\n",
    "            # Assuming all elements are floats, each float uses 8 bytes\n",
    "            output_size_bytes = (output_size * 8) / zfp_comp_ratio\n",
    "            output_size_mbits = (output_size_bytes * 8) / (1024 ** 2)\n",
    "            # All outputs of the layer are the same size, the total size will be (output size * num_output_nodes)\n",
    "            transfer_size = num_outbound * output_size_mbits\n",
    "            transfer_size_dict[partition_points[i]] = transfer_size\n",
    "            transfer_sizes.append(transfer_size)\n",
    "\n",
    "        return transfer_sizes, transfer_size_dict\n",
    "\n",
    "    # For each node, finds the next partition point with the smallest transfer size\n",
    "    def partition_model(self, node_capacities: List[int], communication_graph: nx.Graph):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def distance_to_bandwidth(d):\n",
    "    # Network with average bandwidth = 6.5 Mbps\n",
    "    a = 283230\n",
    "    return math.log2(1 + a / (d ** 2))\n",
    "\n",
    "def get_bottleneck(transfer_sizes: List[int], G_c: nx.Graph, arrangement: List[int]):\n",
    "    bottleneck = 0\n",
    "    for t in range(len(transfer_sizes)):\n",
    "        latency = transfer_sizes[t] / G_c[arrangement[t]][arrangement[t+1]]['weight']\n",
    "        if latency > bottleneck:\n",
    "            bottleneck = latency\n",
    "\n",
    "    return bottleneck\n",
    "\n",
    "def generate_comm_graph(num_nodes: int):\n",
    "    rng = np.random.default_rng()\n",
    "    # Set of arrays of len 2\n",
    "    node_pos = (rng.random((num_nodes, 2)) * 149) + 1\n",
    "    comm_graph = nx.complete_graph(num_nodes)\n",
    "    nodes_list = list(comm_graph.nodes())\n",
    "    for n in range(len(nodes_list)):\n",
    "        comm_graph.nodes()[nodes_list[n]]['pos'] = node_pos[n]\n",
    "    for j in comm_graph.edges():\n",
    "        u = j[0]\n",
    "        v = j[1]\n",
    "        dist = scipy.spatial.distance.euclidean(comm_graph.nodes[u][\"pos\"], comm_graph.nodes[v][\"pos\"])\n",
    "        w = distance_to_bandwidth(dist)\n",
    "        comm_graph[u][v][\"weight\"] = w\n",
    "        comm_graph[u][v]['name'] = f\"{u}-{v}\"\n",
    "\n",
    "    return comm_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def random_partition_place(node_capacity, num_nodes, partitions, transfer_sizes, partition_mems):\n",
    "    splits = []\n",
    "    i = 0\n",
    "    while i < len(partitions):\n",
    "        j = i\n",
    "        while sum(partition_mems[i:j-1]) < node_capacity:\n",
    "                j += 1\n",
    "                if j > len(partitions) - 1:\n",
    "                    break\n",
    "        if j != len(partitions):\n",
    "            splits.append(j)\n",
    "        i = j+1\n",
    "    #print(splits)\n",
    "    #print(len(partitions))\n",
    "\n",
    "    choices = []\n",
    "    for s in range(1, len(splits)):\n",
    "        start = splits[s-1] + 1\n",
    "        end = splits[s]\n",
    "        choice = random.randint(start, end)\n",
    "        choices.append(choice)\n",
    "\n",
    "    random_transfers = []\n",
    "    for c in choices:\n",
    "        random_transfers.append(transfer_sizes[c])\n",
    "\n",
    "    comm_graph = generate_comm_graph(num_nodes)\n",
    "    nodes = [i for i in range(num_nodes)]\n",
    "    num_parts = len(random_transfers) + 1\n",
    "    arrangement = random.sample(nodes, num_parts)\n",
    "\n",
    "    bottleneck = get_bottleneck(random_transfers, comm_graph, arrangement)\n",
    "\n",
    "    return bottleneck"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def test_graph_configs(model, model_name):\n",
    "    partitioner = Partitioner(model)\n",
    "    partitions = partitioner.find_partitions()\n",
    "    transfer_sizes = partitioner.find_partition_transfer_size(partitions)[0]\n",
    "\n",
    "    partition_mems = partitioner.find_partition_memory(partitions)\n",
    "\n",
    "    all_data = {}\n",
    "    # Average of many trials for accuracy\n",
    "    num_trials = 50\n",
    "    for i in range(num_trials):\n",
    "        print(f\"Trial #{i+1}\")\n",
    "        for num_nodes in node_nums:\n",
    "            for c in caps:\n",
    "                # Convert to MB\n",
    "                cap = c * (1024 ** 2)\n",
    "                bottleneck = random_partition_place(cap, num_nodes, partitions, transfer_sizes, partition_mems)\n",
    "\n",
    "                key = f\"{model_name}-{c}-{num_nodes}\"\n",
    "                if i == 0:\n",
    "                    old_avg = 0\n",
    "                else:\n",
    "                    old_avg = all_data[key]\n",
    "\n",
    "                new_avg = old_avg + ((bottleneck - old_avg)/(i+1))\n",
    "                all_data[key] = new_avg\n",
    "\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# The models we're using for the test\n",
    "#model_names = ['ResNet50', 'InceptionResNetV2', 'EfficientNetB1', 'MobileNetV2']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 14:34:22.103829: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-18 14:34:22.103935: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial #1\n",
      "Trial #2\n",
      "Trial #3\n",
      "Trial #4\n",
      "Trial #5\n",
      "Trial #6\n",
      "Trial #7\n",
      "Trial #8\n",
      "Trial #9\n",
      "Trial #10\n",
      "Trial #11\n",
      "Trial #12\n",
      "Trial #13\n",
      "Trial #14\n",
      "Trial #15\n",
      "Trial #16\n",
      "Trial #17\n",
      "Trial #18\n",
      "Trial #19\n",
      "Trial #20\n",
      "Trial #21\n",
      "Trial #22\n",
      "Trial #23\n",
      "Trial #24\n",
      "Trial #25\n",
      "Trial #26\n",
      "Trial #27\n",
      "Trial #28\n",
      "Trial #29\n",
      "Trial #30\n",
      "Trial #31\n",
      "Trial #32\n",
      "Trial #33\n",
      "Trial #34\n",
      "Trial #35\n",
      "Trial #36\n",
      "Trial #37\n",
      "Trial #38\n",
      "Trial #39\n",
      "Trial #40\n",
      "Trial #41\n",
      "Trial #42\n",
      "Trial #43\n",
      "Trial #44\n",
      "Trial #45\n",
      "Trial #46\n",
      "Trial #47\n",
      "Trial #48\n",
      "Trial #49\n",
      "Trial #50\n",
      "InceptionResNetV2\t64\t5\t10.679291662156565\n",
      "InceptionResNetV2\t128\t5\t9.3156382309007\n",
      "InceptionResNetV2\t256\t5\t5.422333755575066\n",
      "InceptionResNetV2\t64\t10\t10.123497794067541\n",
      "InceptionResNetV2\t128\t10\t8.143775696925365\n",
      "InceptionResNetV2\t256\t10\t6.499533996186187\n",
      "InceptionResNetV2\t64\t15\t9.7649540818779\n",
      "InceptionResNetV2\t128\t15\t7.957684960212683\n",
      "InceptionResNetV2\t256\t15\t5.889915445362976\n",
      "InceptionResNetV2\t64\t20\t10.386013656200051\n",
      "InceptionResNetV2\t128\t20\t8.536190297465293\n",
      "InceptionResNetV2\t256\t20\t5.727628568228336\n",
      "InceptionResNetV2\t64\t50\t10.861449701511614\n",
      "InceptionResNetV2\t128\t50\t7.831755872197896\n",
      "InceptionResNetV2\t256\t50\t5.407615750706644\n"
     ]
    }
   ],
   "source": [
    "#caps = [64, 128, 256]\n",
    "caps = [64, 128, 256]\n",
    "# Number of nodes\n",
    "node_nums = [5, 10, 15, 20, 50]\n",
    "\n",
    "model = InceptionResNetV2()\n",
    "model_name = 'InceptionResNetV2'\n",
    "\n",
    "data = test_graph_configs(model, model_name)\n",
    "for k in data:\n",
    "    cols = k.split(\"-\")\n",
    "    key_fmt = \"\\t\".join(cols)\n",
    "    val = data[k]\n",
    "    result = f\"{key_fmt}\\t{val}\"\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}