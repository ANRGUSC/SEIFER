{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# !pip install pydot\n",
    "# !pip install keras\n",
    "# !pip install networkx\n",
    "# !pip install scipy\n",
    "# !pip install tensorflow-macos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from keras.applications import *\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy\n",
    "from itertools import combinations\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, List, Dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Partitioner:\n",
    "    def __init__(self, model: keras.Model):\n",
    "        self.model = model\n",
    "        self.Stack = []\n",
    "        self.visited = {}\n",
    "        # The \"depth\"/level that a certain layer is at\n",
    "        self.layer_level = {}\n",
    "        # The layers at a certain depth/level, where the index of the array is the level\n",
    "        self.levels = []\n",
    "\n",
    "    # Constructs model using shape of start layer as the input (doesn't include start layer in the model)\n",
    "    def _construct_model(self, start, end, part_name=\"part_begin\"):\n",
    "        inpt = keras.Input(tensor=self.model.get_layer(start).output, name=part_name)\n",
    "        outpt = self.model.get_layer(end).output\n",
    "        part = keras.Model(inputs=inpt, outputs=outpt)\n",
    "        return part\n",
    "\n",
    "    def construct_models(self, model: keras.Model, num_nodes: int, num_classes: int, node_capacity: int, G_c: nx.Graph):\n",
    "        partitioner = Partitioner(model)\n",
    "        part_pts = partitioner.find_partitions()\n",
    "        transfers = partitioner.find_partition_transfer_size(part_pts)\n",
    "\n",
    "        partition_mems = partitioner.find_partition_memory(part_pts)\n",
    "        partitions, node_arrangement = partition_and_place(num_nodes, node_capacity, G_c, num_classes, part_pts, transfers, partition_mems)\n",
    "\n",
    "        constructed_models = []\n",
    "        i = 0\n",
    "\n",
    "        # Ignore the dispatcher \"partition\"\n",
    "        for p in partitions[1:]:\n",
    "            # Model input\n",
    "            if p[0] == 0:\n",
    "                start_layer = part_pts[0]\n",
    "            else:\n",
    "                # _construct_model() uses an exclusive start layer but inclusive end layer\n",
    "                start_layer = part_pts[p[0]-1]\n",
    "\n",
    "            # Model output\n",
    "            if p[1] == len(part_pts):\n",
    "                end_layer = part_pts[-1]\n",
    "            else:\n",
    "                # End layer of partition in graph is exclusive, so need to subtract one from end layer index\n",
    "                # to use with _construct_model(), which has inclusive end layer\n",
    "                end_layer = part_pts[p[1]-1]\n",
    "\n",
    "            print(f\"Partition {i}: ({start_layer}, {end_layer})\")\n",
    "\n",
    "            model = self._construct_model(start_layer, end_layer, part_name=f\"part_{i}\")\n",
    "            constructed_models.append(model)\n",
    "            print(\"Partition constructed\")\n",
    "            i += 1\n",
    "\n",
    "        return node_arrangement, constructed_models\n",
    "\n",
    "\n",
    "    # A recursive function used by longest_path. See below\n",
    "    # link for details\n",
    "    # https:#www.geeksforgeeks.org/topological-sorting/\n",
    "    def topological_sort_util(self, v: str):\n",
    "        self.visited[v] = True\n",
    "\n",
    "        # Recur for all the vertices adjacent to this vertex\n",
    "        # list<AdjListNode>::iterator i\n",
    "        for i in self.get_next(v):\n",
    "            if not self.visited[i]:\n",
    "                self.topological_sort_util(i)\n",
    "\n",
    "        # Push current vertex to stack which stores topological\n",
    "        # sort\n",
    "        self.Stack.append(v)\n",
    "\n",
    "    # The function to find longest distances from a given vertex.\n",
    "    # It uses recursive topologicalSortUtil() to get topological\n",
    "    # sorting.\n",
    "    def longest_path(self, s: str) -> List[List[str]]:\n",
    "        for l in self.model.layers:\n",
    "            self.visited[l.name] = False\n",
    "            self.layer_level[l.name] = -1 # Equal to -infty\n",
    "\n",
    "        # Call the recursive helper function to store Topological\n",
    "        # Sort starting from all vertices one by one\n",
    "        for l in self.model.layers:\n",
    "            if not self.visited[l.name]:\n",
    "                self.topological_sort_util(l.name)\n",
    "\n",
    "        # Initialize distances to all vertices as infinite and\n",
    "        # distance to source as 0\n",
    "        self.layer_level[s] = 0\n",
    "\n",
    "        # Process vertices in topological order\n",
    "        while len(self.Stack) > 0:\n",
    "\n",
    "            # Get the next vertex from topological order\n",
    "            u = self.Stack.pop()\n",
    "\n",
    "            # Update distances of all adjacent vertices\n",
    "            # list<AdjListNode>::iterator i\n",
    "            if self.layer_level[u] != -1:\n",
    "                for i in self.get_next(u):\n",
    "                    if self.layer_level[i] < self.layer_level[u] + 1:\n",
    "                        self.layer_level[i] = self.layer_level[u] + 1 # Each edge weighted 1\n",
    "\n",
    "        # Create array of calculated longest distances to layer\n",
    "        layers_at_level = [[]] * len(self.layer_level)\n",
    "        for l in self.model.layers:\n",
    "            if len(layers_at_level[self.layer_level[l.name]]) == 0:\n",
    "                layers_at_level[self.layer_level[l.name]] = []\n",
    "\n",
    "            layers_at_level[self.layer_level[l.name]].append(l.name)\n",
    "\n",
    "        return layers_at_level\n",
    "\n",
    "    def find_singletons(self):\n",
    "        # Model only has 1 input, which is input_names[0]\n",
    "        name = self.model.input_names[0]\n",
    "        # Finding the longest path from the start to every other layer\n",
    "        self.levels = self.longest_path(name)\n",
    "        singletons = []\n",
    "        for l in range(len(self.levels)):\n",
    "            if len(self.levels[l]) == 1:\n",
    "                singletons.append(self.levels[l][0])\n",
    "        return singletons\n",
    "\n",
    "    def find_all_paths_util(self, u, d, visited, path, all_paths):\n",
    "        # If the distance of the current path is greater than the longest path (the \"level\") to the destination node, we know the destination node can't be a partition point\n",
    "        if self.layer_level[u] > self.layer_level[d]:\n",
    "            return False\n",
    "        # Mark the current node as visited and store in path\n",
    "        visited[u] = True\n",
    "        path.append(u)\n",
    "\n",
    "        # If current vertex is same as destination, then print\n",
    "        # current path[] (because we've found a path from u to d)\n",
    "        if u == d:\n",
    "            exists = False\n",
    "            # See if path already exists in list of paths\n",
    "            for p in all_paths:\n",
    "                if p == path:\n",
    "                    exists = True\n",
    "                    break\n",
    "\n",
    "            if not exists:\n",
    "                all_paths.append(path.copy())\n",
    "        else:\n",
    "            # If current vertex is not destination\n",
    "            # Recur for all the vertices adjacent to this vertex\n",
    "            for i in self.get_next(u):\n",
    "                if not visited[i]:\n",
    "                    ret = self.find_all_paths_util(i, d, visited, path, all_paths)\n",
    "                    if not ret:\n",
    "                        return False\n",
    "\n",
    "        # Remove current vertex from path[] and mark it as unvisited\n",
    "        path.pop()\n",
    "        visited[u] = False\n",
    "        return True\n",
    "\n",
    "    # Finds all paths from 's' to 'd.' Returns false if a there exists a path from s that has a greater \"level\" than d, otherwise returns true\n",
    "    def find_all_paths(self, s, d) -> bool:\n",
    "        # Mark all the vertices as not visited\n",
    "        visited = {}\n",
    "        for l in self.model.layers:\n",
    "            visited[l.name] = False\n",
    "\n",
    "        # Create an array to store paths\n",
    "        path = []\n",
    "        all_paths = []\n",
    "\n",
    "        # Call the recursive helper function to find all paths\n",
    "        return self.find_all_paths_util(s, d, visited, path, all_paths)\n",
    "\n",
    "    def partitions_util(self, prev, singleton_nodes, partitions):\n",
    "        # Reached the end of the model and found all the partitions\n",
    "        if len(singleton_nodes) == 0:\n",
    "            return partitions\n",
    "        p = False\n",
    "        i = -1 # So first i starts at 0\n",
    "        # Starting from the previous partition point, we iterate through all the subsequent singleton nodes to find the next partition point\n",
    "        while not p:\n",
    "            i += 1\n",
    "            p = self.find_all_paths(prev, singleton_nodes[i])\n",
    "\n",
    "        partitions.append(singleton_nodes[i])\n",
    "        return self.partitions_util(singleton_nodes[i], singleton_nodes[i + 1:], partitions)\n",
    "\n",
    "    def find_partitions(self) -> List[str]:\n",
    "        inpt = self.model.input_names[0]\n",
    "        return self.partitions_util(inpt, self.find_singletons(), [])\n",
    "\n",
    "    def keras_model_memory_usage_in_bytes(self, model, batch_size: int):\n",
    "        \"\"\"\n",
    "        Return the estimated memory usage of a given Keras model in bytes.\n",
    "        This includes the model weights and layers, but excludes the dataset.\n",
    "\n",
    "        The model shapes are multiplied by the batch size, but the weights are not.\n",
    "\n",
    "        Args:\n",
    "            model: A Keras model.\n",
    "            batch_size: The batch size you intend to run the model with. If you\n",
    "                have already specified the batch size in the model itself, then\n",
    "                pass `1` as the argument here.\n",
    "        Returns:\n",
    "            An estimate of the Keras model's memory usage in bytes.\n",
    "\n",
    "        \"\"\"\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.Model):\n",
    "                internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                    layer, batch_size=batch_size\n",
    "                )\n",
    "            single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n",
    "            out_shape = layer.output_shape\n",
    "            if isinstance(out_shape, list):\n",
    "                out_shape = out_shape[0]\n",
    "            for s in out_shape:\n",
    "                if s is None:\n",
    "                    continue\n",
    "                single_layer_mem *= s\n",
    "            shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def keras_layer_memory(self, layer_name, batch_size: int):\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "\n",
    "        if isinstance(layer_name, tf.keras.Model):\n",
    "            internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                layer_name, batch_size=batch_size\n",
    "            )\n",
    "        single_layer_mem = tf.as_dtype(layer_name.dtype or default_dtype).size\n",
    "        out_shape = layer_name.output_shape\n",
    "        if isinstance(out_shape, list):\n",
    "            out_shape = out_shape[0]\n",
    "        for s in out_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def find_partition_memory(self, partition_points):\n",
    "        part_mems = []\n",
    "        #Each index represents the memory between that part pt and the next one\n",
    "        for i in range(1, len(partition_points)):\n",
    "            # Going backwards along layers within partition to find total memory usage\n",
    "            start = self.layer_level[partition_points[i]]\n",
    "            end = self.layer_level[partition_points[i - 1]]\n",
    "            mem = 0\n",
    "            for j in range(start, end, -1):\n",
    "                for l in self.levels[j]:\n",
    "                    layer_mem = self.keras_layer_memory(self.model.get_layer(l), 1)\n",
    "                    mem += layer_mem\n",
    "            part_mems.append(mem)\n",
    "        # Nothing used after last partition pt, which is output layer\n",
    "        part_mems.append(0)\n",
    "        return part_mems\n",
    "\n",
    "    # Returns transfer size of partition in Mbits\n",
    "    def find_partition_transfer_size(self, partition_points) -> List[int]:\n",
    "        transfer_sizes = []\n",
    "        input_size = 1\n",
    "        # Iterate through all elements of shape tuple except first one (which is batch size)\n",
    "        for s in self.model.input.get_shape()[1:]:\n",
    "            input_size *= s\n",
    "        # Compression ratio is ~1.44 (according to https://www.researchgate.net/publication/264417607_Fixed-Rate_Compressed_Floating-Point_Arrays)\n",
    "        zfp_comp_ratio = 1.44\n",
    "        # input_size gives us number of bits, need to convert to bytes\n",
    "        input_size_bytes = (input_size * 8) / zfp_comp_ratio\n",
    "        # Assuming all elements are floats, each float uses 8 bytes\n",
    "        input_size_mbits = (input_size_bytes * 8) / (1024 ** 2)\n",
    "\n",
    "        # Put input size as first element of transfer size array\n",
    "        transfer_sizes.append(input_size_mbits)\n",
    "\n",
    "        for i in range(len(partition_points)):\n",
    "            num_outbound = len(self.model.get_layer(partition_points[i]).outbound_nodes)\n",
    "\n",
    "            # Iterate through all elements of shape tuple except first one (which is batch size)\n",
    "            output_size = 1\n",
    "            for s in self.model.get_layer(partition_points[i]).get_output_at(0).get_shape()[1:]:\n",
    "                output_size *= s\n",
    "\n",
    "            # Assuming all elements are floats, each float uses 8 bytes\n",
    "            output_size_bytes = (output_size * 8) / zfp_comp_ratio\n",
    "            output_size_mbits = (output_size_bytes * 8) / (1024 ** 2)\n",
    "            # All outputs of the layer are the same size, the total size will be (output size * num_output_nodes)\n",
    "            transfer_size = num_outbound * output_size_mbits\n",
    "            transfer_sizes.append(transfer_size)\n",
    "\n",
    "        return transfer_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = ResNet50()\n",
    "# p = Partitioner(model)\n",
    "# #print(dir(model.get_layer(\"conv3_block3_out\")))\n",
    "# #print(model.get_layer(\"conv3_block3_out\").output_shape)\n",
    "# a = p._construct_model(\"conv3_block3_out\", \"avg_pool\")\n",
    "# keras.utils.plot_model(a, to_file = \"partition.png\")\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(a)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.target_spec.supported_types = [tf.float16]\n",
    "# tflite_model = converter.convert()\n",
    "# print(a.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def k_path_color_coding(graph: nx.Graph, k: int):\n",
    "    # Creates speedup of algorithm\n",
    "    a = 1.3\n",
    "    for i in range(int(10*(math.e ** k))):\n",
    "        rng = np.random.default_rng()\n",
    "        coloring = rng.integers(1, (a*k)+1, len(graph.nodes()))\n",
    "        j = 0\n",
    "        for v in graph.nodes():\n",
    "            graph.nodes()[v][\"color\"] = coloring[j]\n",
    "            j += 1\n",
    "        g = {}\n",
    "        for v in graph.nodes():\n",
    "            g[v] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['hasPath'] = True\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['path'] = [v]\n",
    "            for c in range(1, k+1):\n",
    "                if c != graph.nodes()[v]['color']:\n",
    "                    g[v][frozenset([c])] = {}\n",
    "                    g[v][frozenset([c])]['hasPath'] = False\n",
    "        K = range(1, k+1)\n",
    "        for s in range(1, k):\n",
    "            possible_S = list(combinations(K, s))\n",
    "            for u in graph.nodes():\n",
    "                for v in nx.neighbors(graph, u):\n",
    "                    for S in possible_S:\n",
    "                        Sset = frozenset(S)\n",
    "                        if Sset in g[u] and g[u][Sset]['hasPath'] == True:\n",
    "                            if graph.nodes()[v]['color'] not in Sset:\n",
    "                                newSet = list(S).copy()\n",
    "                                newSet.append(graph.nodes()[v]['color'])\n",
    "                                g[v][frozenset(newSet)] = {}\n",
    "                                g[v][frozenset(newSet)]['hasPath'] = True\n",
    "                                newPath = g[u][frozenset(S)]['path'].copy()\n",
    "                                newPath.append(v)\n",
    "                                g[v][frozenset(newSet)]['path'] = newPath\n",
    "\n",
    "        for u in graph.nodes():\n",
    "            if frozenset(K) in g[u]:\n",
    "                if g[u][frozenset(K)]['hasPath']:\n",
    "                    return g[u][frozenset(K)]['path']\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def modified_k_path_color_coding(graph: nx.Graph, k: int, start, end):\n",
    "    # Creates speedup of algorithm\n",
    "    a = 1.3\n",
    "    for i in range(int(10*(math.e ** k))):\n",
    "        rng = np.random.default_rng()\n",
    "        coloring = rng.integers(1, (a*k)+1, len(graph.nodes()))\n",
    "        j = 0\n",
    "        for v in graph.nodes():\n",
    "            graph.nodes()[v][\"color\"] = coloring[j]\n",
    "            j += 1\n",
    "        g = {}\n",
    "        for v in graph.nodes():\n",
    "            g[v] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['hasPath'] = True\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['path'] = [v]\n",
    "            for c in range(1, k+1):\n",
    "                if c != graph.nodes()[v]['color']:\n",
    "                    g[v][frozenset([c])] = {}\n",
    "                    g[v][frozenset([c])]['hasPath'] = False\n",
    "        K = range(1, k+1)\n",
    "        for s in range(1, k):\n",
    "            possible_S = list(combinations(K, s))\n",
    "            for u in graph.nodes():\n",
    "                if start is not None and s == 1 and u != start:\n",
    "                    continue\n",
    "                for v in nx.neighbors(graph, u):\n",
    "                    if v is not None and v == end and s != k-1:\n",
    "                        continue\n",
    "                    for S in possible_S:\n",
    "                        Sset = frozenset(S)\n",
    "                        if Sset in g[u] and g[u][Sset]['hasPath'] == True:\n",
    "                            if graph.nodes[v]['color'] not in Sset:\n",
    "                                newSet = list(S).copy()\n",
    "                                newSet.append(graph.nodes()[v]['color'])\n",
    "                                g[v][frozenset(newSet)] = {}\n",
    "                                g[v][frozenset(newSet)]['hasPath'] = True\n",
    "                                newPath = g[u][frozenset(S)]['path'].copy()\n",
    "                                newPath.append(v)\n",
    "                                g[v][frozenset(newSet)]['path'] = newPath\n",
    "\n",
    "        if end is not None:\n",
    "            if frozenset(K) in g[end]:\n",
    "                if g[end][frozenset(K)]['hasPath']:\n",
    "                    return g[end][frozenset(K)]['path']\n",
    "\n",
    "        else:\n",
    "            for u in graph.nodes():\n",
    "                if frozenset(K) in g[u]:\n",
    "                    if g[u][frozenset(K)]['hasPath']:\n",
    "                        return g[u][frozenset(K)]['path']\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def threshold(X: int, edges, classes: Dict[Tuple, int], t: int):\n",
    "    for e in edges:\n",
    "        name = e[2]['name']\n",
    "        if e[2]['weight'] < t:\n",
    "            classes[name] = X-1\n",
    "        else:\n",
    "            classes[name] = X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def subgraph_k_path(G: nx.Graph, X: int, k: int):\n",
    "    # For the binary search we want the edge in reverse order\n",
    "    edge_list = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "    low = 0\n",
    "    high = len(edge_list)\n",
    "    classes = {}\n",
    "    best_path = []\n",
    "    while low < high:\n",
    "        median = (low+high) // 2\n",
    "        med_weight = edge_list[median][2]['weight']\n",
    "        threshold(X, edge_list, classes, med_weight)\n",
    "        x_edges = [(e[0], e[1]) for e in edge_list if classes[e[2]['name']] == X]\n",
    "        G_x = G.edge_subgraph(x_edges).copy()\n",
    "        result = k_path_color_coding(G_x, k)\n",
    "        if not result:\n",
    "            low = median + 1\n",
    "        else:\n",
    "            high = median\n",
    "            best_path = result\n",
    "\n",
    "    G.remove_nodes_from(best_path)\n",
    "    return best_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def modified_subgraph_k_path(G: nx.Graph, X: int, k: int, s, u):\n",
    "    # For the binary search we want the edge in reverse order\n",
    "    edge_list = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "    low = 0\n",
    "    high = len(edge_list)\n",
    "    classes = {}\n",
    "    best_path = []\n",
    "    while low < high:\n",
    "        median = (low+high) // 2\n",
    "        med_weight = edge_list[median][2]['weight']\n",
    "        threshold(X, edge_list, classes, med_weight)\n",
    "        x_edges = [(e[0], e[1]) for e in edge_list if classes[e[2]['name']] == X]\n",
    "        G_x = G.edge_subgraph(x_edges).copy()\n",
    "        if s is not None and s not in G_x:\n",
    "            low = median + 1\n",
    "            continue\n",
    "        if u is not None and u not in G_x:\n",
    "            low = median + 1\n",
    "            continue\n",
    "        result = modified_k_path_color_coding(G_x, k, s, u)\n",
    "        if result == False or len(result) == 0:\n",
    "            low = median + 1\n",
    "        else:\n",
    "            high = median\n",
    "            best_path = result\n",
    "\n",
    "    G.remove_nodes_from(best_path)\n",
    "    return best_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def find_subarrays(S, X):\n",
    "    x = np.array(S)\n",
    "    a = x == X\n",
    "    inds = [i for i in range(len(S))]\n",
    "    splits = np.split(inds, np.where(np.diff(a)!=0)[0]+1)\n",
    "    subs = [s for s in splits if S[s[0]] == X]\n",
    "    return subs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def classify(transfer_sizes: List[int], chosen_sizes: List[int], num_bins):\n",
    "    bins = np.histogram_bin_edges(transfer_sizes, bins=num_bins)\n",
    "    # Returns the class that each transfer size belongs to\n",
    "    classes = np.digitize(chosen_sizes, bins)\n",
    "    return classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def create_partition_graph(node_capacity: int, partitions: List[str], transfer_sizes, partition_mems):\n",
    "    partitions_dag = nx.DiGraph()\n",
    "    for i in range(len(partitions)):\n",
    "        for j in range(i+1, len(partitions)+1):\n",
    "            mem = sum(partition_mems[i:j-1])\n",
    "            # Partition has to fit into node\n",
    "            if mem < node_capacity:\n",
    "                node_name = f\"{i}-{j}\"\n",
    "                # End layer of partition is exclusive\n",
    "                partitions_dag.add_node(node_name, partition=(i, j))\n",
    "\n",
    "    for n1 in partitions_dag.nodes(data=True):\n",
    "        for n2 in partitions_dag.nodes(data=True):\n",
    "            n1_name = n1[0]\n",
    "            n2_name = n2[0]\n",
    "            uEnd = n1[1]['partition'][1]\n",
    "            vStart = n2[1]['partition'][0]\n",
    "            if uEnd == vStart:\n",
    "                w = transfer_sizes[uEnd]\n",
    "                partitions_dag.add_edge(n1_name, n2_name, weight=w)\n",
    "    return partitions_dag, transfer_sizes\n",
    "\n",
    "path_from = {}\n",
    "def min_cost_path(G, v):\n",
    "    # Node is leaf node\n",
    "    if len(G[v]) == 0:\n",
    "        return [v], 0\n",
    "\n",
    "    # Not actually the last layer, its the layer after the last\n",
    "    partition_last_layer = G.nodes()[v]['partition'][1]\n",
    "    if partition_last_layer not in path_from:\n",
    "        min_path = []\n",
    "        min_cost = math.inf\n",
    "        for c in G[v]:\n",
    "            path, cost = min_cost_path(G, c)\n",
    "            if cost < min_cost:\n",
    "                min_cost = cost\n",
    "                min_path = path\n",
    "\n",
    "        path_from[partition_last_layer] = (min_path, min_cost)\n",
    "\n",
    "    min_path, min_cost = path_from[partition_last_layer]\n",
    "\n",
    "    # The child that resulted in the min cost path\n",
    "    chosen_node = min_path[0]\n",
    "    # Path starting at v and going to a leaf\n",
    "    new_path = [v]\n",
    "    new_path.extend(min_path)\n",
    "    new_cost = G[v][chosen_node]['weight'] + min_cost\n",
    "    return new_path, new_cost\n",
    "\n",
    "def partition(G, transfer_sizes: List, num_nodes: int, num_bins: int):\n",
    "    roots = []\n",
    "    for n in G.nodes():\n",
    "        if G.in_degree(n) == 0:\n",
    "            roots.append(n)\n",
    "\n",
    "    min_path = []\n",
    "    min_cost = math.inf\n",
    "    for r in roots:\n",
    "        path, cost = min_cost_path(G, r)\n",
    "        if len(path) > num_nodes:\n",
    "            continue\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "            min_path = path\n",
    "\n",
    "    # The dispatcher \"partition\" transfer size always has to be the first\n",
    "    chosen_transfer_sizes = [transfer_sizes[0]]\n",
    "    for p in range(len(min_path)-1):\n",
    "        ts = G[min_path[p]][min_path[p+1]]['weight']\n",
    "        chosen_transfer_sizes.append(ts)\n",
    "\n",
    "    # The dispatcher \"partition\" always has to be the first\n",
    "    chosen_partitions = [(0, 0)]\n",
    "    for m in min_path:\n",
    "        chosen_partitions.append(G.nodes()[m]['partition'])\n",
    "\n",
    "    transfer_size_classes = classify(transfer_sizes, chosen_transfer_sizes, num_bins)\n",
    "    return chosen_partitions, transfer_size_classes, chosen_transfer_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def partition_and_place(num_nodes: int, node_capacity: int, comm_graph: nx.Graph, num_classes, partitions, transfers, partition_mems):\n",
    "    G_p, transfer_sizes = create_partition_graph(node_capacity, partitions, transfers, partition_mems)\n",
    "    Q, S, transfer_size_weights = partition(G_p, transfer_sizes, num_nodes, num_classes)\n",
    "    # Q only has the dispatcher\n",
    "    if len(Q) == 1:\n",
    "        raise MemoryError(\"Can't partition with specified number of nodes and capacity\")\n",
    "    # Q only has the dispatcher and one partition\n",
    "    if len(Q) == 2:\n",
    "        raise NotImplementedError(\"Only one partition necessary\")\n",
    "\n",
    "    G_c = comm_graph.copy()\n",
    "    N = k_path_matching(G_c, Q, S, num_classes)\n",
    "    # Rare case, usually if there's too many bandwidth classes\n",
    "    if None in N:\n",
    "        raise NotImplementedError(\"Couldn't find matching\")\n",
    "    return Q, N"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def k_path_matching(g: nx.Graph, Q: List[str], S: List[int], C: int):\n",
    "    original_graph = g.copy()\n",
    "    N = [None] * len(Q)\n",
    "    for X in range(C, 0, -1):\n",
    "        x_paths = find_subarrays(S, X)\n",
    "        x_paths = sorted(x_paths, key=lambda x: len(x), reverse=True)\n",
    "        for j in range(len(x_paths)):\n",
    "            start_idx = x_paths[j][0]\n",
    "            end_idx = start_idx + len(x_paths[j])\n",
    "            start_v = N[start_idx]\n",
    "            end_v = N[end_idx]\n",
    "            if start_v is not None and start_v not in g:\n",
    "                nodes_to_add = list(g.nodes())\n",
    "                nodes_to_add.append(start_v)\n",
    "                g = original_graph.subgraph(nodes_to_add).copy()\n",
    "            if end_v is not None and end_v not in g:\n",
    "                nodes_to_add = list(g.nodes())\n",
    "                nodes_to_add.append(end_v)\n",
    "                g = original_graph.subgraph(nodes_to_add).copy()\n",
    "\n",
    "            path = modified_subgraph_k_path(g, X, len(x_paths[j]) + 1, start_v, end_v)\n",
    "            N[start_idx:start_idx+len(path)] = path\n",
    "\n",
    "    return N"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Node capacities in MB\n",
    "caps = [64, 128, 256, 512]\n",
    "# Number of nodes\n",
    "node_nums = [5, 10, 15, 20, 50]\n",
    "#node_nums = [50]\n",
    "# Num classes: [2, 5, 8, 11, 14, 17, 20]\n",
    "class_nums = [i for i in range(2, 21, 3)]\n",
    "\n",
    "def distance_to_bandwidth(d):\n",
    "    # Network with average bandwidth = 6.5 Mbps\n",
    "    a = 283230\n",
    "    return math.log2(1 + a / (d ** 2))\n",
    "\n",
    "def get_bottleneck(transfer_sizes, G_c, arrangement):\n",
    "    bottleneck = 0\n",
    "    for t in range(len(transfer_sizes)):\n",
    "        latency = transfer_sizes[t] / G_c[arrangement[t]][arrangement[t+1]]['weight']\n",
    "        if latency > bottleneck:\n",
    "            bottleneck = latency\n",
    "\n",
    "    return bottleneck\n",
    "\n",
    "def generate_comm_graph(num_nodes: int):\n",
    "    rng = np.random.default_rng()\n",
    "    # Set of arrays of len 2\n",
    "    node_pos = (rng.random((num_nodes, 2)) * 149) + 1\n",
    "    comm_graph = nx.complete_graph(num_nodes)\n",
    "    nodes_list = list(comm_graph.nodes())\n",
    "    for n in range(len(nodes_list)):\n",
    "        comm_graph.nodes()[nodes_list[n]]['pos'] = node_pos[n]\n",
    "    for j in comm_graph.edges():\n",
    "        u = j[0]\n",
    "        v = j[1]\n",
    "        dist = scipy.spatial.distance.euclidean(comm_graph.nodes[u][\"pos\"], comm_graph.nodes[v][\"pos\"])\n",
    "        w = distance_to_bandwidth(dist)\n",
    "        comm_graph[u][v][\"weight\"] = w\n",
    "        comm_graph[u][v]['name'] = f\"{u}-{v}\"\n",
    "\n",
    "    return comm_graph\n",
    "\n",
    "def test_graph_configs(model, model_name):\n",
    "    partitioner = Partitioner(model)\n",
    "    partitions = partitioner.find_partitions()\n",
    "    transfers = partitioner.find_partition_transfer_size(partitions)\n",
    "\n",
    "    partition_mems = partitioner.find_partition_memory(partitions)\n",
    "\n",
    "    all_data = {}\n",
    "    # Average of many trials for accuracy\n",
    "    num_trials = 50\n",
    "    for i in range(num_trials):\n",
    "        print(f\"Trial #{i+1}\")\n",
    "        for num_nodes in node_nums:\n",
    "            for c in caps:\n",
    "                # Convert to MB\n",
    "                cap = c * (1024 ** 2)\n",
    "                for num_classes in class_nums:\n",
    "                    try:\n",
    "                        #print(f\"{model_name}-{c}-{num_nodes}-{num_classes}\")\n",
    "                        comm_graph = generate_comm_graph(num_nodes)\n",
    "                        transfer_sizes, arrangement = partition_and_place(num_nodes, cap, comm_graph, num_classes, partitions, transfers, partition_mems)\n",
    "                        bottleneck = get_bottleneck(transfer_sizes, comm_graph, arrangement)\n",
    "                    except NotImplementedError as e:\n",
    "                        bottleneck = 0\n",
    "                    except MemoryError as e:\n",
    "                        bottleneck = math.inf\n",
    "\n",
    "                    key = f\"{model_name}-{c}-{num_nodes}-{num_classes}\"\n",
    "                    if i == 0:\n",
    "                        old_avg = 0\n",
    "                    else:\n",
    "                        old_avg = all_data[key]\n",
    "                    new_avg = old_avg + ((bottleneck - old_avg)/(i+1))\n",
    "                    all_data[key] = new_avg\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# The models we're using for the test\n",
    "#model_names = ['ResNet50', 'InceptionResNetV2', 'EfficientNetB1', 'MobileNetV2']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# model =\n",
    "# model_name =\n",
    "# data = test_graph_configs(model, model_name)\n",
    "# for k in data:\n",
    "#     cols = k.split(\"-\")\n",
    "#     key_fmt = \"\\t\".join(cols)\n",
    "#     val = data[k]\n",
    "#     result = f\"{key_fmt}\\t{val}\"\n",
    "#     print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def check_optimality(g: nx.Graph, N: List[int], T: List[float]):\n",
    "    # Assumes that this link becomes the bottleneck (could end up with a case of a small transfer size\n",
    "    # w/ rly small bandwidth that has higher latency than this edge)\n",
    "    max_edge = max(g.edges(data=True), key=lambda x: x[2]['weight'])\n",
    "    t = T.index(max(T))\n",
    "    if (N[t], N[t+1]) == (max_edge[0], max_edge[1]):\n",
    "        bottleneck = get_bottleneck(T, g, N)\n",
    "        min_bottleneck = T[t] / g[N[t]][N[t+1]]['weight']\n",
    "        if bottleneck == min_bottleneck:\n",
    "            return True\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Compare to optimality of joint optimization?\n",
    "# num_trials = 1000\n",
    "# graph_size = 50\n",
    "# model = MobileNetV2()\n",
    "# capacity = 64 * (1024 ** 2)\n",
    "# bw_classes = 20\n",
    "#\n",
    "# partitioner = Partitioner(model)\n",
    "# partitions = partitioner.find_partitions()\n",
    "# transfers = partitioner.find_partition_transfer_size(partitions)[0]\n",
    "#\n",
    "# partition_mems = partitioner.find_partition_memory(partitions)\n",
    "# num_true = 0\n",
    "# for i in range(num_trials):\n",
    "#     if i % 100 == 0:\n",
    "#         print(f\"Trial {i}\")\n",
    "#     comm_graph = generate_comm_graph(graph_size)\n",
    "#     arrangement, transfer_sizes = partition_and_place(model, graph_size, capacity, comm_graph, bw_classes, partitions, transfers, partition_mems)\n",
    "#     if check_optimality(comm_graph, arrangement, transfer_sizes):\n",
    "#         num_true += 1\n",
    "#\n",
    "# print(f\"{num_true} out of {num_trials}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 7), (7, 16), (16, 34), (34, 41)]\n",
      "Partition 0: (input_1, pool1_pool)\n",
      "Partition constructed\n",
      "Partition 1: (pool1_pool, conv3_block2_add)\n",
      "Partition constructed\n",
      "Partition 2: (conv3_block2_add, conv5_block1_add)\n",
      "Partition constructed\n",
      "Partition 3: (conv5_block1_add, predictions)\n",
      "Partition constructed\n",
      "[10, 28, 42, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "graph_size = 50\n",
    "test_model = ResNet50()\n",
    "capacity = 64 * (1024 ** 2)\n",
    "bw_classes = 20\n",
    "\n",
    "comm_graph = generate_comm_graph(graph_size)\n",
    "partitioner = Partitioner(test_model)\n",
    "# First node will be dispatcher node\n",
    "nodes, partitioned_models = partitioner.construct_models(test_model, graph_size, bw_classes, capacity, comm_graph)\n",
    "print(nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 16:13:52.097193: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 624ms/step\n",
      "Single device inference:  [('n06359193', 'web_site', 0.067092866)]\n",
      "Inferencing with part 0\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Inferencing with part 1\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Inferencing with part 2\n",
      "1/1 [==============================] - 13s 13s/step\n",
      "Inferencing with part 3\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2bf6c8ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Distributed inference:  [('n06359193', 'web_site', 0.067092866)]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet import preprocess_input, decode_predictions\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "# Random pixels, just to get a sample output\n",
    "x = np.random.random_sample(input_shape).astype(np.float32)\n",
    "dims_exp = np.expand_dims(x, axis=0)\n",
    "arr = preprocess_input(dims_exp)\n",
    "\n",
    "pred = test_model.predict(arr)\n",
    "print(\"Single device inference: \", decode_predictions(pred, top=1)[0])\n",
    "\n",
    "out = arr\n",
    "for i in range(len(partitioned_models)):\n",
    "    print(f\"Inferencing with part {i}\")\n",
    "    part = partitioned_models[i]\n",
    "    #part.summary()\n",
    "    out = part.predict(out)\n",
    "\n",
    "print(\"Distributed inference: \", decode_predictions(out, top=1)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}