{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from keras.applications import *\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, List, Dict\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Partitioner:\n",
    "    def __init__(self, model: keras.Model):\n",
    "        self.model = model\n",
    "        self.Stack = []\n",
    "        self.visited = {}\n",
    "        # The \"depth\"/level that a certain layer is at\n",
    "        self.layer_level = {}\n",
    "        # The layers at a certain depth/level, where the index of the array is the level\n",
    "        self.levels = []\n",
    "\n",
    "    def get_previous(self, layer_name):\n",
    "        inbound = self.model.get_layer(layer_name).inbound_nodes[0].inbound_layers\n",
    "        if type(inbound) != list:\n",
    "            inbound = [inbound]\n",
    "        return [layer.name for layer in inbound]\n",
    "\n",
    "    def get_next(self, layer_name):\n",
    "        outbound = self.model.get_layer(layer_name).outbound_nodes\n",
    "        return [node.outbound_layer.name for node in outbound]\n",
    "\n",
    "    # Traverses the model starting from layer_name all the way to start\n",
    "    def traverse(self, layer_name, start, part_name, inpt):\n",
    "        # On subsequent recursive steps, the new input layer will be defined,\n",
    "        # so that name needs to be checked in base case\n",
    "        if (layer_name == start) or (layer_name == part_name):\n",
    "            return inpt\n",
    "\n",
    "        output = []\n",
    "        for n in self.get_previous(layer_name):\n",
    "            output.append(self.traverse(n, start, part_name, inpt))\n",
    "\n",
    "        # If the DAG node only has 1 previous connection\n",
    "        if len(output) == 1:\n",
    "            output = output[0]\n",
    "\n",
    "        layer = self.model.get_layer(layer_name)\n",
    "        to_next = layer(output)\n",
    "        return to_next\n",
    "\n",
    "    def construct_model(self, start, end, part_name=\"part_begin\"):\n",
    "        inpt = keras.Input(tensor=self.model.get_layer(start).output, name=part_name)\n",
    "        output = self.traverse(end, start, part_name, inpt)\n",
    "        part = keras.Model(inputs=self.model.get_layer(start).output, outputs=output)\n",
    "        return part\n",
    "\n",
    "    # TODO write this function\n",
    "    def create_model_partitions(self, node_capacities: List[int], communication_graph: nx.Graph):\n",
    "        node_partition_names = self.partition_model(node_capacities, communication_graph)\n",
    "        model_partitions = {}\n",
    "        for k in node_partition_names:\n",
    "            start_layer, end_layer = node_partition_names[k]\n",
    "            model = self.construct_model(start_layer, end_layer)\n",
    "            model_partitions[k] = model\n",
    "            print(\"Model constructed\")\n",
    "\n",
    "        return model_partitions\n",
    "\n",
    "\n",
    "    # A recursive function used by longest_path. See below\n",
    "    # link for details\n",
    "    # https:#www.geeksforgeeks.org/topological-sorting/\n",
    "    def topological_sort_util(self, v: str):\n",
    "        self.visited[v] = True\n",
    "\n",
    "        # Recur for all the vertices adjacent to this vertex\n",
    "        # list<AdjListNode>::iterator i\n",
    "        for i in self.get_next(v):\n",
    "            if not self.visited[i]:\n",
    "                self.topological_sort_util(i)\n",
    "\n",
    "        # Push current vertex to stack which stores topological\n",
    "        # sort\n",
    "        self.Stack.append(v)\n",
    "\n",
    "    # The function to find longest distances from a given vertex.\n",
    "    # It uses recursive topologicalSortUtil() to get topological\n",
    "    # sorting.\n",
    "    def longest_path(self, s: str) -> List[List[str]]:\n",
    "        for l in self.model.layers:\n",
    "            self.visited[l.name] = False\n",
    "            self.layer_level[l.name] = -1 # Equal to -infty\n",
    "\n",
    "        # Call the recursive helper function to store Topological\n",
    "        # Sort starting from all vertices one by one\n",
    "        for l in self.model.layers:\n",
    "            if not self.visited[l.name]:\n",
    "                self.topological_sort_util(l.name)\n",
    "\n",
    "        # Initialize distances to all vertices as infinite and\n",
    "        # distance to source as 0\n",
    "        self.layer_level[s] = 0\n",
    "\n",
    "        # Process vertices in topological order\n",
    "        while len(self.Stack) > 0:\n",
    "\n",
    "            # Get the next vertex from topological order\n",
    "            u = self.Stack.pop()\n",
    "\n",
    "            # Update distances of all adjacent vertices\n",
    "            # list<AdjListNode>::iterator i\n",
    "            if self.layer_level[u] != -1:\n",
    "                for i in self.get_next(u):\n",
    "                    if self.layer_level[i] < self.layer_level[u] + 1:\n",
    "                        self.layer_level[i] = self.layer_level[u] + 1 # Each edge weighted 1\n",
    "\n",
    "        # Create array of calculated longest distances to layer\n",
    "        layers_at_level = [[]] * len(self.layer_level)\n",
    "        for l in self.model.layers:\n",
    "            if len(layers_at_level[self.layer_level[l.name]]) == 0:\n",
    "                layers_at_level[self.layer_level[l.name]] = []\n",
    "\n",
    "            layers_at_level[self.layer_level[l.name]].append(l.name)\n",
    "\n",
    "        return layers_at_level\n",
    "\n",
    "    def find_singletons(self):\n",
    "        # Model only has 1 input, which is input_names[0]\n",
    "        name = self.model.input_names[0]\n",
    "        # Finding the longest path from the start to every other layer\n",
    "        self.levels = self.longest_path(name)\n",
    "        singletons = []\n",
    "        for l in range(len(self.levels)):\n",
    "            if len(self.levels[l]) == 1:\n",
    "                singletons.append(self.levels[l][0])\n",
    "        return singletons\n",
    "\n",
    "    def find_all_paths_util(self, u, d, visited, path, all_paths):\n",
    "        # If the distance of the current path is greater than the longest path (the \"level\") to the destination node, we know the destination node can't be a partition point\n",
    "        if self.layer_level[u] > self.layer_level[d]:\n",
    "            return False\n",
    "        # Mark the current node as visited and store in path\n",
    "        visited[u] = True\n",
    "        path.append(u)\n",
    "\n",
    "        # If current vertex is same as destination, then print\n",
    "        # current path[] (because we've found a path from u to d)\n",
    "        if u == d:\n",
    "            exists = False\n",
    "            # See if path already exists in list of paths\n",
    "            for p in all_paths:\n",
    "                if p == path:\n",
    "                    exists = True\n",
    "                    break\n",
    "\n",
    "            if not exists:\n",
    "                all_paths.append(path.copy())\n",
    "        else:\n",
    "            # If current vertex is not destination\n",
    "            # Recur for all the vertices adjacent to this vertex\n",
    "            for i in self.get_next(u):\n",
    "                if not visited[i]:\n",
    "                    ret = self.find_all_paths_util(i, d, visited, path, all_paths)\n",
    "                    if not ret:\n",
    "                        return False\n",
    "\n",
    "        # Remove current vertex from path[] and mark it as unvisited\n",
    "        path.pop()\n",
    "        visited[u] = False\n",
    "        return True\n",
    "\n",
    "    # Finds all paths from 's' to 'd.' Returns false if a there exists a path from s that has a greater \"level\" than d, otherwise returns true\n",
    "    def find_all_paths(self, s, d) -> bool:\n",
    "        # Mark all the vertices as not visited\n",
    "        visited = {}\n",
    "        for l in self.model.layers:\n",
    "            visited[l.name] = False\n",
    "\n",
    "        # Create an array to store paths\n",
    "        path = []\n",
    "        all_paths = []\n",
    "\n",
    "        # Call the recursive helper function to find all paths\n",
    "        return self.find_all_paths_util(s, d, visited, path, all_paths)\n",
    "\n",
    "    def partitions_util(self, prev, singleton_nodes, partitions):\n",
    "        # Reached the end of the model and found all the partitions\n",
    "        if len(singleton_nodes) == 0:\n",
    "            return partitions\n",
    "        p = False\n",
    "        i = -1 # So first i starts at 0\n",
    "        # Starting from the previous partition point, we iterate through all the subsequent singleton nodes to find the next partition point\n",
    "        while not p:\n",
    "            i += 1\n",
    "            p = self.find_all_paths(prev, singleton_nodes[i])\n",
    "\n",
    "        partitions.append(singleton_nodes[i])\n",
    "        return self.partitions_util(singleton_nodes[i], singleton_nodes[i + 1:], partitions)\n",
    "\n",
    "    def find_partitions(self) -> List[str]:\n",
    "        inpt = self.model.input_names[0]\n",
    "        return self.partitions_util(inpt, self.find_singletons(), [])\n",
    "\n",
    "    def keras_model_memory_usage_in_bytes(self, model, batch_size: int):\n",
    "        \"\"\"\n",
    "        Return the estimated memory usage of a given Keras model in bytes.\n",
    "        This includes the model weights and layers, but excludes the dataset.\n",
    "\n",
    "        The model shapes are multiplied by the batch size, but the weights are not.\n",
    "\n",
    "        Args:\n",
    "            model: A Keras model.\n",
    "            batch_size: The batch size you intend to run the model with. If you\n",
    "                have already specified the batch size in the model itself, then\n",
    "                pass `1` as the argument here.\n",
    "        Returns:\n",
    "            An estimate of the Keras model's memory usage in bytes.\n",
    "\n",
    "        \"\"\"\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.Model):\n",
    "                internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                    layer, batch_size=batch_size\n",
    "                )\n",
    "            single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n",
    "            out_shape = layer.output_shape\n",
    "            if isinstance(out_shape, list):\n",
    "                out_shape = out_shape[0]\n",
    "            for s in out_shape:\n",
    "                if s is None:\n",
    "                    continue\n",
    "                single_layer_mem *= s\n",
    "            shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def keras_layer_memory(self, layer_name, batch_size: int):\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "\n",
    "        if isinstance(layer_name, tf.keras.Model):\n",
    "            internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                layer_name, batch_size=batch_size\n",
    "            )\n",
    "        single_layer_mem = tf.as_dtype(layer_name.dtype or default_dtype).size\n",
    "        out_shape = layer_name.output_shape\n",
    "        if isinstance(out_shape, list):\n",
    "            out_shape = out_shape[0]\n",
    "        for s in out_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def find_partition_memory(self, partition_points):\n",
    "        part_mems = []\n",
    "        #Each index represents the memory between that part pt and the next one\n",
    "        for i in range(1, len(partition_points)):\n",
    "            # Going backwards along layers within partition to find total memory usage\n",
    "            start = self.layer_level[partition_points[i]]\n",
    "            end = self.layer_level[partition_points[i - 1]]\n",
    "            mem = 0\n",
    "            for j in range(start, end, -1):\n",
    "                for l in self.levels[j]:\n",
    "                    layer_mem = self.keras_layer_memory(self.model.get_layer(l), 1)\n",
    "                    mem += layer_mem\n",
    "            part_mems.append(mem)\n",
    "        # Nothing used after last partition pt, which is output layer\n",
    "        part_mems.append(0)\n",
    "        return part_mems\n",
    "\n",
    "    # Returns transfer size of partition in Mbits\n",
    "    def find_partition_transfer_size(self, partition_points) -> Tuple[List[int], Dict[str, int]]:\n",
    "        transfer_sizes = []\n",
    "        transfer_size_dict = {}\n",
    "        for i in range(len(partition_points)):\n",
    "            num_outbound = len(self.model.get_layer(partition_points[i]).outbound_nodes)\n",
    "\n",
    "            # Iterate through all elements of shape tuple except first one (which is batch size)\n",
    "            output_size = 1\n",
    "            for s in self.model.get_layer(partition_points[i]).get_output_at(0).get_shape()[1:]:\n",
    "                output_size *= s\n",
    "            # Compression ratio is ~1.44 (according to https://www.researchgate.net/publication/264417607_Fixed-Rate_Compressed_Floating-Point_Arrays)\n",
    "            zfp_comp_ratio = 1.44\n",
    "            # Assuming all elements are floats, each float uses 8 bytes\n",
    "            output_size_bytes = (output_size * 8) / zfp_comp_ratio\n",
    "            output_size_mbits = (output_size_bytes * 8) / (1024 ** 2)\n",
    "            # All outputs of the layer are the same size, the total size will be (output size * num_output_nodes)\n",
    "            transfer_size = num_outbound * output_size_mbits\n",
    "            transfer_size_dict[partition_points[i]] = transfer_size\n",
    "            transfer_sizes.append(transfer_size)\n",
    "\n",
    "        return transfer_sizes, transfer_size_dict\n",
    "\n",
    "    # For each node, finds the next partition point with the smallest transfer size\n",
    "    def partition_model(self, node_capacities: List[int], communication_graph: nx.Graph):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def k_path_color_coding(graph: nx.Graph, k: int):\n",
    "    # Creates speedup of algorithm\n",
    "    a = 1.3\n",
    "    for i in range(int(10*(math.e ** k))):\n",
    "        rng = np.random.default_rng()\n",
    "        coloring = rng.integers(1, (a*k)+1, len(graph.nodes()))\n",
    "        j = 0\n",
    "        for v in graph.nodes():\n",
    "            graph.nodes()[v][\"color\"] = coloring[j]\n",
    "            j += 1\n",
    "        g = {}\n",
    "        for v in graph.nodes():\n",
    "            g[v] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['hasPath'] = True\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['path'] = [v]\n",
    "            for c in range(1, k+1):\n",
    "                if c != graph.nodes()[v]['color']:\n",
    "                    g[v][frozenset([c])] = {}\n",
    "                    g[v][frozenset([c])]['hasPath'] = False\n",
    "        K = range(1, k+1)\n",
    "        for s in range(1, k):\n",
    "            possible_S = list(combinations(K, s))\n",
    "            for u in graph.nodes():\n",
    "                for v in nx.neighbors(graph, u):\n",
    "                    for S in possible_S:\n",
    "                        Sset = frozenset(S)\n",
    "                        if Sset in g[u] and g[u][Sset]['hasPath'] == True:\n",
    "                            if graph.nodes()[v]['color'] not in Sset:\n",
    "                                newSet = list(S).copy()\n",
    "                                newSet.append(graph.nodes()[v]['color'])\n",
    "                                g[v][frozenset(newSet)] = {}\n",
    "                                g[v][frozenset(newSet)]['hasPath'] = True\n",
    "                                newPath = g[u][frozenset(S)]['path'].copy()\n",
    "                                newPath.append(v)\n",
    "                                g[v][frozenset(newSet)]['path'] = newPath\n",
    "\n",
    "        for u in graph.nodes():\n",
    "            if frozenset(K) in g[u]:\n",
    "                if g[u][frozenset(K)]['hasPath']:\n",
    "                    return g[u][frozenset(K)]['path']\n",
    "\n",
    "    return False\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def modified_k_path_color_coding(graph: nx.Graph, k: int, start, end):\n",
    "    # Creates speedup of algorithm\n",
    "    a = 1.3\n",
    "    for i in range(int(10*(math.e ** k))):\n",
    "        rng = np.random.default_rng()\n",
    "        coloring = rng.integers(1, (a*k)+1, len(graph.nodes()))\n",
    "        j = 0\n",
    "        for v in graph.nodes():\n",
    "            graph.nodes()[v][\"color\"] = coloring[j]\n",
    "            j += 1\n",
    "        g = {}\n",
    "        for v in graph.nodes():\n",
    "            g[v] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])] = {}\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['hasPath'] = True\n",
    "            g[v][frozenset([graph.nodes()[v]['color']])]['path'] = [v]\n",
    "            for c in range(1, k+1):\n",
    "                if c != graph.nodes()[v]['color']:\n",
    "                    g[v][frozenset([c])] = {}\n",
    "                    g[v][frozenset([c])]['hasPath'] = False\n",
    "        K = range(1, k+1)\n",
    "        for s in range(1, k):\n",
    "            possible_S = list(combinations(K, s))\n",
    "            for u in graph.nodes():\n",
    "                if start is not None and s == 1 and u != start:\n",
    "                    continue\n",
    "                for v in nx.neighbors(graph, u):\n",
    "                    if v is not None and v == end and s != k-1:\n",
    "                        continue\n",
    "                    for S in possible_S:\n",
    "                        Sset = frozenset(S)\n",
    "                        if Sset in g[u] and g[u][Sset]['hasPath'] == True:\n",
    "                            if graph.nodes[v]['color'] not in Sset:\n",
    "                                newSet = list(S).copy()\n",
    "                                newSet.append(graph.nodes()[v]['color'])\n",
    "                                g[v][frozenset(newSet)] = {}\n",
    "                                g[v][frozenset(newSet)]['hasPath'] = True\n",
    "                                newPath = g[u][frozenset(S)]['path'].copy()\n",
    "                                newPath.append(v)\n",
    "                                g[v][frozenset(newSet)]['path'] = newPath\n",
    "\n",
    "        if end is not None:\n",
    "            if frozenset(K) in g[end]:\n",
    "                if g[end][frozenset(K)]['hasPath']:\n",
    "                    return g[end][frozenset(K)]['path']\n",
    "\n",
    "        else:\n",
    "            for u in graph.nodes():\n",
    "                if frozenset(K) in g[u]:\n",
    "                    if g[u][frozenset(K)]['hasPath']:\n",
    "                        return g[u][frozenset(K)]['path']\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def threshold(X: int, edges, classes: Dict[Tuple, int], t: int):\n",
    "    for e in edges:\n",
    "        name = e[2]['name']\n",
    "        if e[2]['weight'] < t:\n",
    "            classes[name] = X-1\n",
    "        else:\n",
    "            classes[name] = X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def subgraph_k_path(G: nx.Graph, X: int, k: int):\n",
    "    # For the binary search we want the edge in reverse order\n",
    "    edge_list = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "    low = 0\n",
    "    high = len(edge_list)\n",
    "    classes = {}\n",
    "    best_path = []\n",
    "    while low < high:\n",
    "        median = (low+high) // 2\n",
    "        med_weight = edge_list[median][2]['weight']\n",
    "        threshold(X, edge_list, classes, med_weight)\n",
    "        x_edges = [(e[0], e[1]) for e in edge_list if classes[e[2]['name']] == X]\n",
    "        G_x = G.edge_subgraph(x_edges).copy()\n",
    "        result = k_path_color_coding(G_x, k)\n",
    "        if not result:\n",
    "            low = median + 1\n",
    "        else:\n",
    "            high = median\n",
    "            best_path = result\n",
    "\n",
    "    G.remove_nodes_from(best_path)\n",
    "    return best_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def modified_subgraph_k_path(G: nx.Graph, X: int, k: int, s, u):\n",
    "    # For the binary search we want the edge in reverse order\n",
    "    edge_list = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "    low = 0\n",
    "    high = len(edge_list)\n",
    "    classes = {}\n",
    "    best_path = []\n",
    "    while low < high:\n",
    "        median = (low+high) // 2\n",
    "        med_weight = edge_list[median][2]['weight']\n",
    "        threshold(X, edge_list, classes, med_weight)\n",
    "        x_edges = [(e[0], e[1]) for e in edge_list if classes[e[2]['name']] == X]\n",
    "        G_x = G.edge_subgraph(x_edges).copy()\n",
    "        if s is not None and s not in G_x:\n",
    "            low = median + 1\n",
    "            continue\n",
    "        if u is not None and u not in G_x:\n",
    "            low = median + 1\n",
    "            continue\n",
    "        result = modified_k_path_color_coding(G_x, k, s, u)\n",
    "        if result == False or len(result) == 0:\n",
    "            low = median + 1\n",
    "        else:\n",
    "            high = median\n",
    "            best_path = result\n",
    "\n",
    "    G.remove_nodes_from(best_path)\n",
    "    return best_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def find_subarrays(S, X):\n",
    "    x = np.array(S)\n",
    "    a = x == X\n",
    "    inds = [i for i in range(len(S))]\n",
    "    splits = np.split(inds, np.where(np.diff(a)!=0)[0]+1)\n",
    "    subs = [s for s in splits if S[s[0]] == X]\n",
    "    return subs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def classify(transfer_sizes: List[int], chosen_sizes: List[int], num_bins):\n",
    "    bins = np.histogram_bin_edges(transfer_sizes, bins=num_bins)\n",
    "    # Returns the class that each transfer size belongs to\n",
    "    classes = np.digitize(chosen_sizes, bins)\n",
    "    return classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def create_partition_graph(node_capacity: int, partitions: List[str], transfer_sizes, partition_mems):\n",
    "    partitions_dag = nx.DiGraph()\n",
    "    for i in range(len(partitions)):\n",
    "        for j in range(i+1, len(partitions)+1):\n",
    "            mem = sum(partition_mems[i:j-1])\n",
    "            # Partition has to fit into node\n",
    "            if mem < node_capacity:\n",
    "                node_name = f\"{i}-{j}\"\n",
    "                # End layer of partition is exclusive\n",
    "                partitions_dag.add_node(node_name, partition=(i, j))\n",
    "\n",
    "    for n1 in partitions_dag.nodes(data=True):\n",
    "        for n2 in partitions_dag.nodes(data=True):\n",
    "            n1_name = n1[0]\n",
    "            n2_name = n2[0]\n",
    "            uEnd = n1[1]['partition'][1]\n",
    "            vStart = n2[1]['partition'][0]\n",
    "            if uEnd == vStart:\n",
    "                w = transfer_sizes[uEnd-1]\n",
    "                partitions_dag.add_edge(n1_name, n2_name, weight=w)\n",
    "\n",
    "    return partitions_dag\n",
    "\n",
    "def min_cost_path(G, v, path_from: dict):\n",
    "    # Node is leaf node\n",
    "    if len(G[v]) == 0:\n",
    "        return [v], 0\n",
    "\n",
    "    # Not actually the last layer, its the layer after the last\n",
    "    partition_last_layer = G.nodes()[v]['partition'][1]\n",
    "    if partition_last_layer not in path_from:\n",
    "        min_path = []\n",
    "        min_cost = math.inf\n",
    "        for c in G[v]:\n",
    "            path, cost = min_cost_path(G, c, path_from)\n",
    "            if cost < min_cost:\n",
    "                min_cost = cost\n",
    "                min_path = path\n",
    "\n",
    "        path_from[partition_last_layer] = (min_path, min_cost)\n",
    "\n",
    "    min_path, min_cost = path_from[partition_last_layer]\n",
    "\n",
    "    # The child that resulted in the min cost path\n",
    "    chosen_node = min_path[0]\n",
    "    # Path starting at v and going to a leaf\n",
    "    new_path = [v]\n",
    "    new_path.extend(min_path)\n",
    "    new_cost = G[v][chosen_node]['weight'] + min_cost\n",
    "    return new_path, new_cost\n",
    "\n",
    "def partition(G, transfer_sizes: List, num_nodes: int, num_bins: int):\n",
    "    roots = []\n",
    "    for n in G.nodes():\n",
    "        if G.in_degree(n) == 0:\n",
    "            roots.append(n)\n",
    "\n",
    "    path_from = {}\n",
    "    min_path = []\n",
    "    min_cost = math.inf\n",
    "    for r in roots:\n",
    "        path, cost = min_cost_path(G, r, path_from)\n",
    "        if len(path) > num_nodes:\n",
    "            continue\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "            min_path = path\n",
    "\n",
    "    chosen_sizes = []\n",
    "    for p in range(len(min_path)-1):\n",
    "        ts = G[min_path[p]][min_path[p+1]]['weight']\n",
    "        chosen_sizes.append(ts)\n",
    "\n",
    "    transfer_size_classes = classify(transfer_sizes, chosen_sizes, num_bins)\n",
    "    return min_path, transfer_size_classes, chosen_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def partition_and_place(num_nodes: int, node_capacity: int, comm_graph: nx.Graph, num_classes, partitions, transfers, partition_mems):\n",
    "    G_p = create_partition_graph(node_capacity, partitions, transfers, partition_mems)\n",
    "    Q, S, chosen_transfer_sizes = partition(G_p, transfers, num_nodes, num_classes)\n",
    "    if len(Q) == 0:\n",
    "        raise MemoryError(\"Can't partition with specified number of nodes and capacity\")\n",
    "    if len(Q) == 1:\n",
    "        raise NotImplementedError(\"Only one partition\")\n",
    "    G_c = comm_graph.copy()\n",
    "    N = k_path_matching(G_c, Q, S, num_classes)\n",
    "    # Rare case, usually if there's too many bandwidth classes\n",
    "    if None in N:\n",
    "        raise NotImplementedError(\"Couldn't find matching\")\n",
    "    return N, chosen_transfer_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def k_path_matching(g: nx.Graph, Q: List[str], S: List[int], C: int):\n",
    "    original_graph = g.copy()\n",
    "    N = [None] * len(Q)\n",
    "    for X in range(C, 0, -1):\n",
    "        x_paths = find_subarrays(S, X)\n",
    "        x_paths = sorted(x_paths, key=lambda x: len(x), reverse=True)\n",
    "        for j in range(len(x_paths)):\n",
    "            start_idx = x_paths[j][0]\n",
    "            end_idx = start_idx + len(x_paths[j])\n",
    "            start_v = N[start_idx]\n",
    "            end_v = N[end_idx]\n",
    "            if start_v is not None and start_v not in g:\n",
    "                nodes_to_add = list(g.nodes())\n",
    "                nodes_to_add.append(start_v)\n",
    "                g = original_graph.subgraph(nodes_to_add).copy()\n",
    "            if end_v is not None and end_v not in g:\n",
    "                nodes_to_add = list(g.nodes())\n",
    "                nodes_to_add.append(end_v)\n",
    "                g = original_graph.subgraph(nodes_to_add).copy()\n",
    "\n",
    "            path = modified_subgraph_k_path(g, X, len(x_paths[j]) + 1, start_v, end_v)\n",
    "            N[start_idx:start_idx+len(path)] = path\n",
    "\n",
    "    return N"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Node capacities in MB\n",
    "caps = [64, 128, 256, 512]\n",
    "# Number of nodes\n",
    "node_nums = [5, 10, 15, 20, 50]\n",
    "#node_nums = [50]\n",
    "# Num classes: [2, 5, 8, 11, 14, 17, 20]\n",
    "class_nums = [i for i in range(2, 21, 3)]\n",
    "\n",
    "def distance_to_bandwidth(d):\n",
    "    # Network with average bandwidth = 6.5 Mbps\n",
    "    a = 283230\n",
    "    return math.log2(1 + a / (d ** 2))\n",
    "\n",
    "def get_bottleneck(transfer_sizes, G_c, arrangement):\n",
    "    bottleneck = 0\n",
    "    for t in range(len(transfer_sizes)):\n",
    "        latency = transfer_sizes[t] / G_c[arrangement[t]][arrangement[t+1]]['weight']\n",
    "        if latency > bottleneck:\n",
    "            bottleneck = latency\n",
    "\n",
    "    return bottleneck\n",
    "\n",
    "def generate_comm_graph(num_nodes: int):\n",
    "    rng = np.random.default_rng()\n",
    "    # Set of arrays of len 2\n",
    "    node_pos = (rng.random((num_nodes, 2)) * 149) + 1\n",
    "    comm_graph = nx.complete_graph(num_nodes)\n",
    "    nodes_list = list(comm_graph.nodes())\n",
    "    for n in range(len(nodes_list)):\n",
    "        comm_graph.nodes()[nodes_list[n]]['pos'] = node_pos[n]\n",
    "    for j in comm_graph.edges():\n",
    "        u = j[0]\n",
    "        v = j[1]\n",
    "        dist = scipy.spatial.distance.euclidean(comm_graph.nodes[u][\"pos\"], comm_graph.nodes[v][\"pos\"])\n",
    "        w = distance_to_bandwidth(dist)\n",
    "        comm_graph[u][v][\"weight\"] = w\n",
    "        comm_graph[u][v]['name'] = f\"{u}-{v}\"\n",
    "\n",
    "    return comm_graph\n",
    "\n",
    "def test_graph_configs(model, model_name):\n",
    "    partitioner = Partitioner(model)\n",
    "    partitions = partitioner.find_partitions()\n",
    "    transfers = partitioner.find_partition_transfer_size(partitions)[0]\n",
    "\n",
    "    partition_mems = partitioner.find_partition_memory(partitions)\n",
    "\n",
    "    all_data = {}\n",
    "    # Average of many trials for accuracy\n",
    "    num_trials = 50\n",
    "    for i in range(num_trials):\n",
    "        print(f\"Trial #{i+1}\")\n",
    "        for num_nodes in node_nums:\n",
    "            for c in caps:\n",
    "                # Convert to MB\n",
    "                cap = c * (1024 ** 2)\n",
    "                for num_classes in class_nums:\n",
    "                    try:\n",
    "                        #print(f\"{model_name}-{c}-{num_nodes}-{num_classes}\")\n",
    "                        comm_graph = generate_comm_graph(num_nodes)\n",
    "                        arrangement, transfer_sizes = partition_and_place(num_nodes, cap, comm_graph, num_classes, partitions, transfers, partition_mems)\n",
    "                        bottleneck = get_bottleneck(transfer_sizes, comm_graph, arrangement)\n",
    "                    except NotImplementedError as e:\n",
    "                        bottleneck = 0\n",
    "                    except MemoryError as e:\n",
    "                        bottleneck = math.inf\n",
    "\n",
    "                    key = f\"{model_name}-{c}-{num_nodes}-{num_classes}\"\n",
    "                    if i == 0:\n",
    "                        old_avg = 0\n",
    "                    else:\n",
    "                        old_avg = all_data[key]\n",
    "                    new_avg = old_avg + ((bottleneck - old_avg)/(i+1))\n",
    "                    all_data[key] = new_avg\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# model =\n",
    "# model_name =\n",
    "# data = test_graph_configs(model, model_name)\n",
    "# for k in data:\n",
    "#     cols = k.split(\"-\")\n",
    "#     key_fmt = \"\\t\".join(cols)\n",
    "#     val = data[k]\n",
    "#     result = f\"{key_fmt}\\t{val}\"\n",
    "#     print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_modules():\n",
    "    modules = []\n",
    "    for mod in dir(keras.applications):\n",
    "        # If submodule name is uppercase, it is the class of a model and not a submodule\n",
    "        if mod[0].isupper():\n",
    "            if 'Xception' in mod or 'VGG' in mod:\n",
    "                modules.append(mod)\n",
    "\n",
    "    return modules\n",
    "\n",
    "def get_model(ms: str):\n",
    "    if 'MobileNet' in ms:\n",
    "        model = eval(f\"{ms}(input_shape=(224, 224, 3), weights=\\'imagenet\\')\")\n",
    "    elif 'RegNet' in ms:\n",
    "       model = eval(f\"keras.applications.regnet.{ms}(weights=\\'imagenet\\')\")\n",
    "    else:\n",
    "        model = eval(f\"{ms}(weights=\\'imagenet\\')\")\n",
    "\n",
    "    return ms, model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def check_optimality(g: nx.Graph, N: List[int], T: List[float]):\n",
    "    max_edge = max(g.edges(data=True), key=lambda x: x[2]['weight'])\n",
    "    t = T.index(max(T))\n",
    "    if (N[t], N[t+1]) == (max_edge[0], max_edge[1]):\n",
    "        bottleneck = get_bottleneck(T, g, N)\n",
    "        min_bottleneck = T[t] / g[N[t]][N[t+1]]['weight']\n",
    "        if bottleneck == min_bottleneck:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def get_approx_ratio(g: nx.Graph, N: List[int], T: List[float]):\n",
    "    max_edge = max(g.edges(data=True), key=lambda x: x[2]['weight'])\n",
    "    t = T.index(max(T))\n",
    "    min_bottleneck = T[t] / max_edge[2]['weight']\n",
    "    bottleneck = get_bottleneck(T, g, N)\n",
    "    approx_ratio = bottleneck / min_bottleneck\n",
    "\n",
    "    return approx_ratio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def find_avg_approx_ratios():\n",
    "    num_trials = 1000\n",
    "    graph_size = 50\n",
    "    capacity = 64 * (1024 ** 2)\n",
    "    bw_classes = 20\n",
    "\n",
    "    all_data = {}\n",
    "    modules = get_modules()\n",
    "    for m in range(len(modules)):\n",
    "        model_name, model = get_model(modules[m])\n",
    "        print(f\"Starting {model_name}\")\n",
    "\n",
    "        partitioner = Partitioner(model)\n",
    "        partitions = partitioner.find_partitions()\n",
    "        transfers = partitioner.find_partition_transfer_size(partitions)[0]\n",
    "        partition_mems = partitioner.find_partition_memory(partitions)\n",
    "\n",
    "        avg_ratio = 0\n",
    "        for i in range(num_trials):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Trial {i}\")\n",
    "\n",
    "            communication_graph = generate_comm_graph(graph_size)\n",
    "            try:\n",
    "                arrangement, transfer_sizes = partition_and_place(graph_size, capacity, communication_graph, bw_classes, partitions, transfers, partition_mems)\n",
    "                new_ratio = get_approx_ratio(communication_graph, arrangement, transfer_sizes)\n",
    "                avg_ratio = avg_ratio + ((new_ratio - avg_ratio)/(i+1))\n",
    "            except NotImplementedError as e:\n",
    "                print(f\"{model_name}: {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"{model_name}: {avg_ratio}\")\n",
    "        all_data[model_name] = avg_ratio\n",
    "\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 17:23:42.066166: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-14 17:23:42.066556: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VGG16\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "VGG16: 1.1977752251529954\n",
      "Starting VGG19\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "VGG19: 1.0\n",
      "Starting Xception\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "Xception: 1.2510193762610127\n",
      "VGG16\t1.1977752251529954\n",
      "VGG19\t1.0\n",
      "Xception\t1.2510193762610127\n"
     ]
    }
   ],
   "source": [
    "data = find_avg_approx_ratios()\n",
    "for k in data:\n",
    "    cols = k.split(\"-\")\n",
    "    key_fmt = \"\\t\".join(cols)\n",
    "    val = data[k]\n",
    "    result = f\"{key_fmt}\\t{val}\"\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Proportion of Optimality\n",
    "# MobileNetV2: 455/1000\n",
    "# EfficientNetB1: 446/1000\n",
    "# ResNet50: 0/1000\n",
    "# InceptionResNetV2: 0/1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Avg Approx Ratio for 1000 trials (run this for all Keras models)\n",
    "# DenseNet121:  1.003697197769907\n",
    "# DenseNet169:  1.003956330668004\n",
    "# DenseNet201:\t1.032416277\n",
    "# MobileNetV2:  1.0\n",
    "# EfficientNetB1:   1.0\n",
    "# ResNet50: 1.268173496735394\n",
    "# InceptionResNetV2:    1.486318590632739"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}