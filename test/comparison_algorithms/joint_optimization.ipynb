{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from keras.applications import *\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Partitioner:\n",
    "    def __init__(self, model: keras.Model):\n",
    "        self.model = model\n",
    "        self.Stack = []\n",
    "        self.visited = {}\n",
    "        # The \"depth\"/level that a certain layer is at\n",
    "        self.layer_level = {}\n",
    "        # The layers at a certain depth/level, where the index of the array is the level\n",
    "        self.levels = []\n",
    "\n",
    "    def get_previous(self, layer_name):\n",
    "        inbound = self.model.get_layer(layer_name).inbound_nodes[0].inbound_layers\n",
    "        if type(inbound) != list:\n",
    "            inbound = [inbound]\n",
    "        return [layer.name for layer in inbound]\n",
    "\n",
    "    def get_next(self, layer_name):\n",
    "        outbound = self.model.get_layer(layer_name).outbound_nodes\n",
    "        return [node.outbound_layer.name for node in outbound]\n",
    "\n",
    "    # Traverses the model starting from layer_name all the way to start\n",
    "    def traverse(self, layer_name, start, part_name, inpt):\n",
    "        # On subsequent recursive steps, the new input layer will be defined,\n",
    "        # so that name needs to be checked in base case\n",
    "        if (layer_name == start) or (layer_name == part_name):\n",
    "            return inpt\n",
    "\n",
    "        output = []\n",
    "        for n in self.get_previous(layer_name):\n",
    "            output.append(self.traverse(n, start, part_name, inpt))\n",
    "\n",
    "        # If the DAG node only has 1 previous connection\n",
    "        if len(output) == 1:\n",
    "            output = output[0]\n",
    "\n",
    "        layer = self.model.get_layer(layer_name)\n",
    "        to_next = layer(output)\n",
    "        return to_next\n",
    "\n",
    "    def construct_model(self, start, end, part_name=\"part_begin\"):\n",
    "        inpt = keras.Input(tensor=self.model.get_layer(start).output, name=part_name)\n",
    "        output = self.traverse(end, start, part_name, inpt)\n",
    "        part = keras.Model(inputs=self.model.get_layer(start).output, outputs=output)\n",
    "        return part\n",
    "\n",
    "    # TODO write this function\n",
    "    def create_model_partitions(self, node_capacities: List[str], communication_graph: nx.Graph):\n",
    "        node_partition_names = self.partition_model(node_capacities, communication_graph)\n",
    "        model_partitions = {}\n",
    "        for k in node_partition_names:\n",
    "            start_layer, end_layer = node_partition_names[k]\n",
    "            model = self.construct_model(start_layer, end_layer)\n",
    "            model_partitions[k] = model\n",
    "            print(\"Model constructed\")\n",
    "\n",
    "        return model_partitions\n",
    "\n",
    "\n",
    "    # A recursive function used by longest_path. See below\n",
    "    # link for details\n",
    "    # https:#www.geeksforgeeks.org/topological-sorting/\n",
    "    def topological_sort_util(self, v: str):\n",
    "        self.visited[v] = True\n",
    "\n",
    "        # Recur for all the vertices adjacent to this vertex\n",
    "        # list<AdjListNode>::iterator i\n",
    "        for i in self.get_next(v):\n",
    "            if not self.visited[i]:\n",
    "                self.topological_sort_util(i)\n",
    "\n",
    "        # Push current vertex to stack which stores topological\n",
    "        # sort\n",
    "        self.Stack.append(v)\n",
    "\n",
    "    # The function to find longest distances from a given vertex.\n",
    "    # It uses recursive topologicalSortUtil() to get topological\n",
    "    # sorting.\n",
    "    def longest_path(self, s: str) -> List[List[str]]:\n",
    "        for l in self.model.layers:\n",
    "            self.visited[l.name] = False\n",
    "            self.layer_level[l.name] = -1 # Equal to -infty\n",
    "\n",
    "        # Call the recursive helper function to store Topological\n",
    "        # Sort starting from all vertices one by one\n",
    "        for l in self.model.layers:\n",
    "            if not self.visited[l.name]:\n",
    "                self.topological_sort_util(l.name)\n",
    "\n",
    "        # Initialize distances to all vertices as infinite and\n",
    "        # distance to source as 0\n",
    "        self.layer_level[s] = 0\n",
    "\n",
    "        # Process vertices in topological order\n",
    "        while len(self.Stack) > 0:\n",
    "\n",
    "            # Get the next vertex from topological order\n",
    "            u = self.Stack.pop()\n",
    "\n",
    "            # Update distances of all adjacent vertices\n",
    "            # list<AdjListNode>::iterator i\n",
    "            if self.layer_level[u] != -1:\n",
    "                for i in self.get_next(u):\n",
    "                    if self.layer_level[i] < self.layer_level[u] + 1:\n",
    "                        self.layer_level[i] = self.layer_level[u] + 1 # Each edge weighted 1\n",
    "\n",
    "        # Create array of calculated longest distances to layer\n",
    "        layers_at_level = [[]] * len(self.layer_level)\n",
    "        for l in self.model.layers:\n",
    "            if len(layers_at_level[self.layer_level[l.name]]) == 0:\n",
    "                layers_at_level[self.layer_level[l.name]] = []\n",
    "\n",
    "            layers_at_level[self.layer_level[l.name]].append(l.name)\n",
    "\n",
    "        return layers_at_level\n",
    "\n",
    "    def find_singletons(self):\n",
    "        # Model only has 1 input, which is input_names[0]\n",
    "        name = self.model.input_names[0]\n",
    "        # Finding the longest path from the start to every other layer\n",
    "        self.levels = self.longest_path(name)\n",
    "        singletons = []\n",
    "        for l in range(len(self.levels)):\n",
    "            if len(self.levels[l]) == 1:\n",
    "                singletons.append(self.levels[l][0])\n",
    "        return singletons\n",
    "\n",
    "    def find_all_paths_util(self, u, d, visited, path, all_paths):\n",
    "        # If the distance of the current path is greater than the longest path (the \"level\") to the destination node, we know the destination node can't be a partition point\n",
    "        if self.layer_level[u] > self.layer_level[d]:\n",
    "            return False\n",
    "        # Mark the current node as visited and store in path\n",
    "        visited[u] = True\n",
    "        path.append(u)\n",
    "\n",
    "        # If current vertex is same as destination, then print\n",
    "        # current path[] (because we've found a path from u to d)\n",
    "        if u == d:\n",
    "            exists = False\n",
    "            # See if path already exists in list of paths\n",
    "            for p in all_paths:\n",
    "                if p == path:\n",
    "                    exists = True\n",
    "                    break\n",
    "\n",
    "            if not exists:\n",
    "                all_paths.append(path.copy())\n",
    "        else:\n",
    "            # If current vertex is not destination\n",
    "            # Recur for all the vertices adjacent to this vertex\n",
    "            for i in self.get_next(u):\n",
    "                if not visited[i]:\n",
    "                    ret = self.find_all_paths_util(i, d, visited, path, all_paths)\n",
    "                    if not ret:\n",
    "                        return False\n",
    "\n",
    "        # Remove current vertex from path[] and mark it as unvisited\n",
    "        path.pop()\n",
    "        visited[u] = False\n",
    "        return True\n",
    "\n",
    "    # Finds all paths from 's' to 'd.' Returns false if a there exists a path from s that has a greater \"level\" than d, otherwise returns true\n",
    "    def find_all_paths(self, s, d) -> bool:\n",
    "        # Mark all the vertices as not visited\n",
    "        visited = {}\n",
    "        for l in self.model.layers:\n",
    "            visited[l.name] = False\n",
    "\n",
    "        # Create an array to store paths\n",
    "        path = []\n",
    "        all_paths = []\n",
    "\n",
    "        # Call the recursive helper function to find all paths\n",
    "        return self.find_all_paths_util(s, d, visited, path, all_paths)\n",
    "\n",
    "    def partitions_util(self, prev, singleton_nodes, partitions):\n",
    "        # Reached the end of the model and found all the partitions\n",
    "        if len(singleton_nodes) == 0:\n",
    "            return partitions\n",
    "        p = False\n",
    "        i = -1 # So first i starts at 0\n",
    "        # Starting from the previous partition point, we iterate through all the subsequent singleton nodes to find the next partition point\n",
    "        while not p:\n",
    "            i += 1\n",
    "            p = self.find_all_paths(prev, singleton_nodes[i])\n",
    "\n",
    "        partitions.append(singleton_nodes[i])\n",
    "        return self.partitions_util(singleton_nodes[i], singleton_nodes[i + 1:], partitions)\n",
    "\n",
    "    def find_partitions(self) -> List[str]:\n",
    "        inpt = self.model.input_names[0]\n",
    "        return self.partitions_util(inpt, self.find_singletons(), [])\n",
    "\n",
    "    def keras_model_memory_usage_in_bytes(self, model, batch_size: int):\n",
    "        \"\"\"\n",
    "        Return the estimated memory usage of a given Keras model in bytes.\n",
    "        This includes the model weights and layers, but excludes the dataset.\n",
    "\n",
    "        The model shapes are multiplied by the batch size, but the weights are not.\n",
    "\n",
    "        Args:\n",
    "            model: A Keras model.\n",
    "            batch_size: The batch size you intend to run the model with. If you\n",
    "                have already specified the batch size in the model itself, then\n",
    "                pass `1` as the argument here.\n",
    "        Returns:\n",
    "            An estimate of the Keras model's memory usage in bytes.\n",
    "\n",
    "        \"\"\"\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.Model):\n",
    "                internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                    layer, batch_size=batch_size\n",
    "                )\n",
    "            single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n",
    "            out_shape = layer.output_shape\n",
    "            if isinstance(out_shape, list):\n",
    "                out_shape = out_shape[0]\n",
    "            for s in out_shape:\n",
    "                if s is None:\n",
    "                    continue\n",
    "                single_layer_mem *= s\n",
    "            shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def keras_layer_memory(self, layer_name, batch_size: int):\n",
    "        default_dtype = tf.keras.backend.floatx()\n",
    "        shapes_mem_count = 0\n",
    "        internal_model_mem_count = 0\n",
    "\n",
    "        if isinstance(layer_name, tf.keras.Model):\n",
    "            internal_model_mem_count += self.keras_model_memory_usage_in_bytes(\n",
    "                layer_name, batch_size=batch_size\n",
    "            )\n",
    "        single_layer_mem = tf.as_dtype(layer_name.dtype or default_dtype).size\n",
    "        out_shape = layer_name.output_shape\n",
    "        if isinstance(out_shape, list):\n",
    "            out_shape = out_shape[0]\n",
    "        for s in out_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "        trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.trainable_weights]\n",
    "        )\n",
    "        non_trainable_count = sum(\n",
    "            [tf.keras.backend.count_params(p) for p in layer_name.non_trainable_weights]\n",
    "        )\n",
    "\n",
    "        total_memory = (\n",
    "                batch_size * shapes_mem_count\n",
    "                + internal_model_mem_count\n",
    "                + trainable_count\n",
    "                + non_trainable_count\n",
    "        )\n",
    "        return total_memory\n",
    "\n",
    "    def find_partition_memory(self, partition_points):\n",
    "        part_mems = []\n",
    "        #Each index represents the memory between that part pt and the next one\n",
    "        for i in range(1, len(partition_points)):\n",
    "            # Going backwards along layers within partition to find total memory usage\n",
    "            start = self.layer_level[partition_points[i]]\n",
    "            end = self.layer_level[partition_points[i - 1]]\n",
    "            mem = 0\n",
    "            for j in range(start, end, -1):\n",
    "                for l in self.levels[j]:\n",
    "                    layer_mem = self.keras_layer_memory(self.model.get_layer(l), 1)\n",
    "                    mem += layer_mem\n",
    "            part_mems.append(mem)\n",
    "        # Nothing used after last partition pt, which is output layer\n",
    "        part_mems.append(0)\n",
    "        return part_mems\n",
    "\n",
    "    # Returns transfer size of partition in Mbits\n",
    "    def find_partition_transfer_size(self, partition_points) -> Tuple[List[int], Dict[str, int]]:\n",
    "        transfer_sizes = []\n",
    "        transfer_size_dict = {}\n",
    "        for i in range(len(partition_points)):\n",
    "            num_outbound = len(self.model.get_layer(partition_points[i]).outbound_nodes)\n",
    "\n",
    "            # Iterate through all elements of shape tuple except first one (which is batch size)\n",
    "            output_size = 1\n",
    "            for s in self.model.get_layer(partition_points[i]).get_output_at(0).get_shape()[1:]:\n",
    "                output_size *= s\n",
    "            # Compression ratio is ~1.44 (according to https://www.researchgate.net/publication/264417607_Fixed-Rate_Compressed_Floating-Point_Arrays)\n",
    "            zfp_comp_ratio = 1.44\n",
    "            # Assuming all elements are floats, each float uses 8 bytes\n",
    "            output_size_bytes = (output_size * 8) / zfp_comp_ratio\n",
    "            output_size_mbits = (output_size_bytes * 8) / (1024 ** 2)\n",
    "            # All outputs of the layer are the same size, the total size will be (output size * num_output_nodes)\n",
    "            transfer_size = num_outbound * output_size_mbits\n",
    "            transfer_size_dict[partition_points[i]] = transfer_size\n",
    "            transfer_sizes.append(transfer_size)\n",
    "\n",
    "        return transfer_sizes, transfer_size_dict\n",
    "\n",
    "    # For each node, finds the next partition point with the smallest transfer size\n",
    "    def partition_model(self, node_capacities: List[int], communication_graph: nx.Graph):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_partition_graph(node_capacity: int, partitions: List[str], transfer_sizes, partition_mems):\n",
    "    partitions_dag = nx.DiGraph()\n",
    "    for i in range(len(partitions)):\n",
    "        for j in range(i+1, len(partitions)+1):\n",
    "            mem = sum(partition_mems[i:j-1])\n",
    "            # Partition has to fit into node\n",
    "            if mem < node_capacity:\n",
    "                node_name = f\"{i}-{j}\"\n",
    "                # End layer of partition is exclusive\n",
    "                partitions_dag.add_node(node_name, partition=(i, j))\n",
    "\n",
    "    for n1 in partitions_dag.nodes(data=True):\n",
    "        for n2 in partitions_dag.nodes(data=True):\n",
    "            n1_name = n1[0]\n",
    "            n2_name = n2[0]\n",
    "            uEnd = n1[1]['partition'][1]\n",
    "            vStart = n2[1]['partition'][0]\n",
    "            if uEnd == vStart:\n",
    "                w = transfer_sizes[uEnd-1]\n",
    "                partitions_dag.add_edge(n1_name, n2_name, weight=w)\n",
    "\n",
    "    return partitions_dag\n",
    "\n",
    "\n",
    "def min_cost_path(G, v, path_from: dict):\n",
    "    # Node is leaf node\n",
    "    if len(G[v]) == 0:\n",
    "        return [v], 0\n",
    "\n",
    "    # Not actually the last layer, its the layer after the last\n",
    "    partition_last_layer = G.nodes()[v]['partition'][1]\n",
    "    if partition_last_layer not in path_from:\n",
    "        min_path = []\n",
    "        min_cost = math.inf\n",
    "        for c in G[v]:\n",
    "            path, cost = min_cost_path(G, c, path_from)\n",
    "            if cost < min_cost:\n",
    "                min_cost = cost\n",
    "                min_path = path\n",
    "\n",
    "        path_from[partition_last_layer] = (min_path, min_cost)\n",
    "\n",
    "    min_path, min_cost = path_from[partition_last_layer]\n",
    "\n",
    "    # The child that resulted in the min cost path\n",
    "    chosen_node = min_path[0]\n",
    "    # Path starting at v and going to a leaf\n",
    "    new_path = [v]\n",
    "    new_path.extend(min_path)\n",
    "    new_cost = G[v][chosen_node]['weight'] + min_cost\n",
    "    return new_path, new_cost\n",
    "\n",
    "def partition(G, num_nodes: int):\n",
    "    roots = []\n",
    "    for n in G.nodes():\n",
    "        if G.in_degree(n) == 0:\n",
    "            roots.append(n)\n",
    "\n",
    "    path_from = {}\n",
    "    min_path = []\n",
    "    min_cost = math.inf\n",
    "    for r in roots:\n",
    "        path, cost = min_cost_path(G, r, path_from)\n",
    "        if len(path) > num_nodes:\n",
    "            continue\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "            min_path = path\n",
    "\n",
    "    chosen_sizes = []\n",
    "    for p in range(len(min_path)-1):\n",
    "        ts = G[min_path[p]][min_path[p+1]]['weight']\n",
    "        chosen_sizes.append(ts)\n",
    "\n",
    "    return min_path, chosen_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def distance_to_bandwidth(d):\n",
    "    # Network with average bandwidth = 6.5 Mbps\n",
    "    a = 283230\n",
    "    return math.log2(1 + a / (d ** 2))\n",
    "\n",
    "def get_bottleneck(transfer_sizes, G_c, arrangement):\n",
    "    bottleneck = 0\n",
    "    for t in range(len(transfer_sizes)):\n",
    "        # The communication graph has inverse bandwidth as the weights\n",
    "        latency = transfer_sizes[t] * G_c[arrangement[t]][arrangement[t+1]]['weight']\n",
    "        if latency > bottleneck:\n",
    "            bottleneck = latency\n",
    "\n",
    "    return bottleneck\n",
    "\n",
    "def generate_comm_graph(num_nodes: int):\n",
    "    rng = np.random.default_rng()\n",
    "    # Set of arrays of len 2\n",
    "    node_pos = (rng.random((num_nodes, 2)) * 149) + 1\n",
    "    comm_graph = nx.complete_graph(num_nodes)\n",
    "    nodes_list = list(comm_graph.nodes())\n",
    "    for n in range(len(nodes_list)):\n",
    "        comm_graph.nodes()[nodes_list[n]]['pos'] = node_pos[n]\n",
    "    for j in comm_graph.edges():\n",
    "        u = j[0]\n",
    "        v = j[1]\n",
    "        dist = scipy.spatial.distance.euclidean(comm_graph.nodes[u][\"pos\"], comm_graph.nodes[v][\"pos\"])\n",
    "        w = distance_to_bandwidth(dist)\n",
    "\n",
    "        inv_bandwidth = 1 / w\n",
    "        comm_graph[u][v][\"weight\"] = inv_bandwidth\n",
    "        comm_graph[u][v]['name'] = f\"{u}-{v}\"\n",
    "\n",
    "    return comm_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def partition_and_place(num_nodes: int, node_capacity: int, comm_graph: nx.Graph, partitions, transfers, partition_mems):\n",
    "    part_graph = create_partition_graph(node_capacity, partitions, transfers, partition_mems)\n",
    "    partitions, transfer_sizes = partition(part_graph, num_nodes)\n",
    "\n",
    "    if len(partitions) == 0:\n",
    "        raise MemoryError(\"Can't partition with specified number of nodes and capacity\")\n",
    "    if len(partitions) == 1:\n",
    "        raise NotImplementedError(\"Only one partition\")\n",
    "\n",
    "    G_c = comm_graph.copy()\n",
    "    N = choose_node_path(partitions, transfer_sizes, G_c)\n",
    "\n",
    "    return N, transfer_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def choose_node_path(partitions, transfers, comm_graph: nx.Graph):\n",
    "    best_path = []\n",
    "    best_cost = math.inf\n",
    "    nodes = list(comm_graph.nodes())\n",
    "    for node in nodes:\n",
    "        G_c = comm_graph.copy()\n",
    "        u = node\n",
    "        path = [node]\n",
    "        weights = []\n",
    "        for p in range(len(partitions)-1):\n",
    "            min_weight = math.inf\n",
    "            min_edge = 0\n",
    "            for v in G_c[u]:\n",
    "                weight = G_c[u][v]['weight']\n",
    "                if weight < min_weight:\n",
    "                    min_weight = weight\n",
    "                    min_edge = v\n",
    "\n",
    "            G_c.remove_node(u)\n",
    "            u = min_edge\n",
    "            path.append(min_edge)\n",
    "            weights.append(min_weight)\n",
    "\n",
    "        bottleneck = get_bottleneck(transfers, comm_graph, path)\n",
    "        if bottleneck < best_cost:\n",
    "            best_cost = bottleneck\n",
    "            best_path = path\n",
    "\n",
    "    return best_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def test_graph_configs(model, model_name, node_nums, caps):\n",
    "    partitioner = Partitioner(model)\n",
    "    partitions = partitioner.find_partitions()\n",
    "    transfers = partitioner.find_partition_transfer_size(partitions)[0]\n",
    "\n",
    "    partition_mems = partitioner.find_partition_memory(partitions)\n",
    "\n",
    "    all_data = {}\n",
    "    # Average of many trials for accuracy\n",
    "    num_trials = 50\n",
    "    for i in range(num_trials):\n",
    "        print(f\"Trial #{i+1}\")\n",
    "        for num_nodes in node_nums:\n",
    "            for c in caps:\n",
    "                # Convert to MB\n",
    "                cap = c * (1024 ** 2)\n",
    "                comm_graph = generate_comm_graph(num_nodes)\n",
    "                arrangement, transfer_sizes = partition_and_place(num_nodes, cap, comm_graph, partitions, transfers, partition_mems)\n",
    "                bottleneck = get_bottleneck(transfer_sizes, comm_graph, arrangement)\n",
    "\n",
    "                key = f\"{model_name}-{c}-{num_nodes}\"\n",
    "                if i == 0:\n",
    "                    old_avg = 0\n",
    "                else:\n",
    "                    old_avg = all_data[key]\n",
    "\n",
    "                new_avg = old_avg + ((bottleneck - old_avg)/(i+1))\n",
    "                all_data[key] = new_avg\n",
    "\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# The models we're using for the test\n",
    "#model_names = ['ResNet50', 'InceptionResNetV2', 'EfficientNetB1', 'MobileNetV2']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# caps = [64, 128, 256]\n",
    "# # Number of nodes\n",
    "# node_nums = [5, 10, 15, 20, 50]\n",
    "#\n",
    "# model = InceptionResNetV2()\n",
    "# model_name = 'InceptionResNetV2'\n",
    "#\n",
    "# data = test_graph_configs(model, model_name, node_nums, caps)\n",
    "# for k in data:\n",
    "#     cols = k.split(\"-\")\n",
    "#     key_fmt = \"\\t\".join(cols)\n",
    "#     val = data[k]\n",
    "#     result = f\"{key_fmt}\\t{val}\"\n",
    "#     print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_modules():\n",
    "    modules = []\n",
    "    i = 0\n",
    "    for mod in dir(keras.applications):\n",
    "        # If submodule name is uppercase, it is the class of a model and not a submodule\n",
    "        if mod[0].isupper():\n",
    "            if \"NASNet\" not in mod:\n",
    "                modules.append(mod)\n",
    "\n",
    "    return modules\n",
    "\n",
    "def get_model(ms: str):\n",
    "    if 'MobileNet' in ms:\n",
    "        model = eval(f\"{ms}(input_shape=(224, 224, 3), weights=\\'imagenet\\')\")\n",
    "    elif 'RegNet' in ms:\n",
    "       model = eval(f\"keras.applications.regnet.{ms}(weights=\\'imagenet\\')\")\n",
    "    else:\n",
    "        model = eval(f\"{ms}(weights=\\'imagenet\\')\")\n",
    "\n",
    "    return ms, model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def check_optimality(g: nx.Graph, N: List[int], T: List[float]):\n",
    "    # Max bandwidth edge is the min inverse bandwidth edge\n",
    "    max_edge = min(g.edges(data=True), key=lambda x: x[2]['weight'])\n",
    "    t = T.index(max(T))\n",
    "    if (N[t], N[t+1]) == (max_edge[0], max_edge[1]):\n",
    "        bottleneck = get_bottleneck(T, g, N)\n",
    "        # Communication graph has inverse bandwidth as edge weight so need to multiply\n",
    "        min_bottleneck = T[t] * g[N[t]][N[t+1]]['weight']\n",
    "        if bottleneck == min_bottleneck:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def get_approx_ratio(g: nx.Graph, N: List[int], T: List[float]):\n",
    "    # Max bandwidth edge is the min inverse bandwidth edge\n",
    "    max_edge = min(g.edges(data=True), key=lambda x: x[2]['weight'])\n",
    "    t = T.index(max(T))\n",
    "    # Communication graph has inverse bandwidth as edge weight so need to multiply\n",
    "    min_bottleneck = T[t] * max_edge[2]['weight']\n",
    "    bottleneck = get_bottleneck(T, g, N)\n",
    "    approx_ratio = bottleneck / min_bottleneck\n",
    "\n",
    "    return approx_ratio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def find_avg_approx_ratios():\n",
    "    num_trials = 1000\n",
    "    graph_size = 50\n",
    "    # Test extreme case of 16 and 32 MB node capacity\n",
    "    # (not applicable to real-world but hopefully shows superiority of k-path matching\n",
    "    capacity = 32 * (1024 ** 2)\n",
    "\n",
    "    all_data = {}\n",
    "    modules = get_modules()\n",
    "    for m in range(len(modules)):\n",
    "        model_name, model = get_model(modules[m])\n",
    "        print(f\"Starting {model_name}\")\n",
    "\n",
    "        partitioner = Partitioner(model)\n",
    "        partitions = partitioner.find_partitions()\n",
    "        transfers = partitioner.find_partition_transfer_size(partitions)[0]\n",
    "        partition_mems = partitioner.find_partition_memory(partitions)\n",
    "\n",
    "        avg_ratio = 0\n",
    "        for i in range(num_trials):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Trial {i}\")\n",
    "\n",
    "            communication_graph = generate_comm_graph(graph_size)\n",
    "            try:\n",
    "                arrangement, transfer_sizes = partition_and_place(graph_size, capacity, communication_graph, partitions, transfers, partition_mems)\n",
    "                new_ratio = get_approx_ratio(communication_graph, arrangement, transfer_sizes)\n",
    "                avg_ratio = avg_ratio + ((new_ratio - avg_ratio)/(i+1))\n",
    "            except NotImplementedError as e:\n",
    "                print(f\"{model_name}: {e}\")\n",
    "                break\n",
    "            except MemoryError as e:\n",
    "                print(f\"{model_name}: {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"{model_name}: {avg_ratio}\")\n",
    "        all_data[model_name] = avg_ratio\n",
    "\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ConvNeXtBase\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ConvNeXtBase: 1.0026878117628886\n",
      "Starting ConvNeXtLarge\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ConvNeXtLarge: 1.3888357177335608\n",
      "Starting ConvNeXtSmall\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ConvNeXtSmall: 1.0025442548781405\n",
      "Starting ConvNeXtTiny\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ConvNeXtTiny: 1.0026479139373412\n",
      "Starting ConvNeXtXLarge\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ConvNeXtXLarge: 1.3887064392513522\n",
      "Starting DenseNet121\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "DenseNet121: 1.0314655292833215\n",
      "Starting DenseNet169\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "DenseNet169: 1.0310257569742627\n",
      "Starting DenseNet201\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "DenseNet201: 1.0409434316158106\n",
      "Starting EfficientNetB0\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetB0: 1.000104129062324\n",
      "Starting EfficientNetB1\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetB1: 1.0000680114562255\n",
      "Starting EfficientNetB2\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetB2: 1.0784523781361257\n",
      "Starting EfficientNetB3\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetB3: 1.041054600235502\n",
      "Starting EfficientNetB4\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetB4: 1.0549639056529163\n",
      "Starting EfficientNetB5\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetB5: 1.2961815210770375\n",
      "Starting EfficientNetB6\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetB6: 1.4212205699141778\n",
      "Starting EfficientNetB7\n",
      "Trial 0\n",
      "EfficientNetB7: Can't partition with specified number of nodes and capacity\n",
      "EfficientNetB7: 0\n",
      "Starting EfficientNetV2B0\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetV2B0: 1.0021732419828113\n",
      "Starting EfficientNetV2B1\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetV2B1: 1.0\n",
      "Starting EfficientNetV2B2\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetV2B2: 1.0\n",
      "Starting EfficientNetV2B3\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetV2B3: 1.0015937164832762\n",
      "Starting EfficientNetV2L\n",
      "Trial 0\n",
      "EfficientNetV2L: Can't partition with specified number of nodes and capacity\n",
      "EfficientNetV2L: 0\n",
      "Starting EfficientNetV2M\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetV2M: 1.0420591154940744\n",
      "Starting EfficientNetV2S\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "EfficientNetV2S: 1.379321414099175\n",
      "Starting InceptionResNetV2\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "InceptionResNetV2: 1.4738510970012346\n",
      "Starting InceptionV3\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "InceptionV3: 1.1070071454071713\n",
      "Starting MobileNet\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "MobileNet: 1.0\n",
      "Starting MobileNetV2\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "MobileNetV2: 1.0020768459414926\n",
      "Starting MobileNetV3Large\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "MobileNetV3Large: 1.0\n",
      "Starting MobileNetV3Small\n",
      "Trial 0\n",
      "MobileNetV3Small: Only one partition\n",
      "MobileNetV3Small: 0\n",
      "Starting RegNetX002\n",
      "Trial 0\n",
      "RegNetX002: Only one partition\n",
      "RegNetX002: 0\n",
      "Starting RegNetX004\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX004: 1.0\n",
      "Starting RegNetX006\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX006: 1.0\n",
      "Starting RegNetX008\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX008: 1.0\n",
      "Starting RegNetX016\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX016: 1.0152939123739178\n",
      "Starting RegNetX032\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX032: 1.001860820997633\n",
      "Starting RegNetX040\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX040: 1.07329634871825\n",
      "Starting RegNetX064\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX064: 1.0356553909954667\n",
      "Starting RegNetX080\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX080: 1.1338671504282516\n",
      "Starting RegNetX120\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX120: 1.147348933664714\n",
      "Starting RegNetX160\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX160: 1.2595655415259852\n",
      "Starting RegNetX320\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetX320: 1.2853533377430644\n",
      "Starting RegNetY002\n",
      "Trial 0\n",
      "RegNetY002: Only one partition\n",
      "RegNetY002: 0\n",
      "Starting RegNetY004\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY004: 1.0\n",
      "Starting RegNetY006\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY006: 1.0\n",
      "Starting RegNetY008\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY008: 1.0245557740554383\n",
      "Starting RegNetY016\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY016: 1.0920926343019182\n",
      "Starting RegNetY032\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY032: 1.0763461237008698\n",
      "Starting RegNetY040\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY040: 1.0\n",
      "Starting RegNetY064\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY064: 1.0095563517254518\n",
      "Starting RegNetY080\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY080: 1.0362421473618366\n",
      "Starting RegNetY120\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY120: 1.1486513112861962\n",
      "Starting RegNetY160\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY160: 1.163655309681918\n",
      "Starting RegNetY320\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "RegNetY320: 1.136118001987526\n",
      "Starting ResNet101\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNet101: 1.0390113870292323\n",
      "Starting ResNet101V2\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNet101V2: 1.5163192324512262\n",
      "Starting ResNet152\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNet152: 1.0377173743243686\n",
      "Starting ResNet152V2\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNet152V2: 1.0653589572269617\n",
      "Starting ResNet50\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNet50: 1.0412429997263302\n",
      "Starting ResNet50V2\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNet50V2: 1.2606277990855865\n",
      "Starting ResNetRS101\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNetRS101: 1.2813074211354454\n",
      "Starting ResNetRS152\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNetRS152: 1.2853744333266923\n",
      "Starting ResNetRS200\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNetRS200: 1.2850934527448643\n",
      "Starting ResNetRS270\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNetRS270: 1.4177754012311532\n",
      "Starting ResNetRS350\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNetRS350: 1.4704220027518304\n",
      "Starting ResNetRS420\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNetRS420: 1.0444798796953312\n",
      "Starting ResNetRS50\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "ResNetRS50: 1.2842439416175973\n",
      "Starting VGG16\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "VGG16: 1.028403787057483\n",
      "Starting VGG19\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "VGG19: 1.03473939512274\n",
      "Starting Xception\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "Xception: 1.0031421829987204\n",
      "ConvNeXtBase\t1.0026878117628886\n",
      "ConvNeXtLarge\t1.3888357177335608\n",
      "ConvNeXtSmall\t1.0025442548781405\n",
      "ConvNeXtTiny\t1.0026479139373412\n",
      "ConvNeXtXLarge\t1.3887064392513522\n",
      "DenseNet121\t1.0314655292833215\n",
      "DenseNet169\t1.0310257569742627\n",
      "DenseNet201\t1.0409434316158106\n",
      "EfficientNetB0\t1.000104129062324\n",
      "EfficientNetB1\t1.0000680114562255\n",
      "EfficientNetB2\t1.0784523781361257\n",
      "EfficientNetB3\t1.041054600235502\n",
      "EfficientNetB4\t1.0549639056529163\n",
      "EfficientNetB5\t1.2961815210770375\n",
      "EfficientNetB6\t1.4212205699141778\n",
      "EfficientNetB7\t0\n",
      "EfficientNetV2B0\t1.0021732419828113\n",
      "EfficientNetV2B1\t1.0\n",
      "EfficientNetV2B2\t1.0\n",
      "EfficientNetV2B3\t1.0015937164832762\n",
      "EfficientNetV2L\t0\n",
      "EfficientNetV2M\t1.0420591154940744\n",
      "EfficientNetV2S\t1.379321414099175\n",
      "InceptionResNetV2\t1.4738510970012346\n",
      "InceptionV3\t1.1070071454071713\n",
      "MobileNet\t1.0\n",
      "MobileNetV2\t1.0020768459414926\n",
      "MobileNetV3Large\t1.0\n",
      "MobileNetV3Small\t0\n",
      "RegNetX002\t0\n",
      "RegNetX004\t1.0\n",
      "RegNetX006\t1.0\n",
      "RegNetX008\t1.0\n",
      "RegNetX016\t1.0152939123739178\n",
      "RegNetX032\t1.001860820997633\n",
      "RegNetX040\t1.07329634871825\n",
      "RegNetX064\t1.0356553909954667\n",
      "RegNetX080\t1.1338671504282516\n",
      "RegNetX120\t1.147348933664714\n",
      "RegNetX160\t1.2595655415259852\n",
      "RegNetX320\t1.2853533377430644\n",
      "RegNetY002\t0\n",
      "RegNetY004\t1.0\n",
      "RegNetY006\t1.0\n",
      "RegNetY008\t1.0245557740554383\n",
      "RegNetY016\t1.0920926343019182\n",
      "RegNetY032\t1.0763461237008698\n",
      "RegNetY040\t1.0\n",
      "RegNetY064\t1.0095563517254518\n",
      "RegNetY080\t1.0362421473618366\n",
      "RegNetY120\t1.1486513112861962\n",
      "RegNetY160\t1.163655309681918\n",
      "RegNetY320\t1.136118001987526\n",
      "ResNet101\t1.0390113870292323\n",
      "ResNet101V2\t1.5163192324512262\n",
      "ResNet152\t1.0377173743243686\n",
      "ResNet152V2\t1.0653589572269617\n",
      "ResNet50\t1.0412429997263302\n",
      "ResNet50V2\t1.2606277990855865\n",
      "ResNetRS101\t1.2813074211354454\n",
      "ResNetRS152\t1.2853744333266923\n",
      "ResNetRS200\t1.2850934527448643\n",
      "ResNetRS270\t1.4177754012311532\n",
      "ResNetRS350\t1.4704220027518304\n",
      "ResNetRS420\t1.0444798796953312\n",
      "ResNetRS50\t1.2842439416175973\n",
      "VGG16\t1.028403787057483\n",
      "VGG19\t1.03473939512274\n",
      "Xception\t1.0031421829987204\n"
     ]
    }
   ],
   "source": [
    "data = find_avg_approx_ratios()\n",
    "for k in data:\n",
    "    cols = k.split(\"-\")\n",
    "    key_fmt = \"\\t\".join(cols)\n",
    "    val = data[k]\n",
    "    result = f\"{key_fmt}\\t{val}\"\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}